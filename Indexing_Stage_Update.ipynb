{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfc21c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompt(pth):\n",
    "    with open(pth, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        return content    \n",
    "\n",
    "def process_entity_extraction_prompt(content):\n",
    "    DEFAULT_TUPLE_DELIMITER = \"<|>\"\n",
    "    DEFAULT_RECORD_DELIMITER = \"##\"\n",
    "    DEFAULT_COMPLETION_DELIMITER = \"<|COMPLETE|>\"\n",
    "    content = content.replace('{completion_delimiter}', DEFAULT_COMPLETION_DELIMITER)\n",
    "    content = content.replace('{tuple_delimiter}', DEFAULT_TUPLE_DELIMITER)\n",
    "    content = content.replace('{record_delimiter}', DEFAULT_RECORD_DELIMITER)\n",
    "    return content\n",
    "    \n",
    "    \n",
    "def load_document(folder_path):\n",
    "    # Initialize a list to store the contents of each text file\n",
    "    text_files_content = []\n",
    "    # Loop through each file in the directory\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Check if the file is a text file\n",
    "        if filename.endswith('.txt'):\n",
    "            # Construct full file path\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            # Open the file and read its contents\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                text_files_content.append(content)\n",
    "    return text_files_content\n",
    "\n",
    "\n",
    "def split_documents_into_chunks(documents, chunk_size=2000, overlap_size=400):\n",
    "    \"\"\"\n",
    "    Splits documents into chunks of approximately chunk_size characters,\n",
    "    ensuring each chunk ends at a sentence boundary with an overlap of overlap_size characters.\n",
    "    \n",
    "    :param documents: List of documents (strings) to be split.\n",
    "    :param chunk_size: Approximate maximum size of each chunk in characters.\n",
    "    :param overlap_size: Desired overlap between chunks in characters.\n",
    "    :return: List of chunks.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    for document in documents:\n",
    "        sentences = sent_tokenize(document)\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "        i = 0\n",
    "        while i < len(sentences):\n",
    "            sentence = sentences[i]\n",
    "            sentence_length = len(sentence) + 1  # +1 for space or punctuation\n",
    "            if current_length + sentence_length > chunk_size:\n",
    "                # Add the current chunk to chunks\n",
    "                chunks.append(' '.join(current_chunk).strip())\n",
    "                # Start new chunk with overlap\n",
    "                overlap_sentences = []\n",
    "                overlap_length = 0\n",
    "                # Collect sentences for overlap from the end of current_chunk\n",
    "                j = len(current_chunk) - 1\n",
    "                while j >= 0 and overlap_length < overlap_size:\n",
    "                    overlap_sentences.insert(0, current_chunk[j])\n",
    "                    overlap_length += len(current_chunk[j]) + 1\n",
    "                    j -= 1\n",
    "                current_chunk = overlap_sentences\n",
    "                current_length = overlap_length\n",
    "            else:\n",
    "                current_chunk.append(sentence)\n",
    "                current_length += sentence_length\n",
    "                i += 1\n",
    "        if current_chunk:\n",
    "            chunks.append(' '.join(current_chunk).strip())\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def entity_type_extraction(LLM, prompt, document, domain_name):\n",
    "    LLM_engine, LLM_setting = LLM\n",
    "    prompt = prompt.replace('{domain}', domain_name)\n",
    "    prompt = prompt.replace('{document}', document)\n",
    "\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]    \n",
    "    response = LLM_engine(messages, max_new_tokens=LLM_setting[\"entity_type_extraction_length\"], \n",
    "                   pad_token_id=LLM_setting[\"pad_token_id\"], do_sample = LLM_setting[\"do_sample\"],\n",
    "                    temperature=LLM_setting[\"temperature\"])\n",
    "    entities_types = response[0]['generated_text'][-1]['content']\n",
    "    torch.cuda.empty_cache()\n",
    "    return entities_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b7b8116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_elements_from_chunks(LLM, chunks, extraction_prompt, max_gleanings=3):\n",
    "    \n",
    "    LLM_engine, LLM_setting = LLM\n",
    "    CONTINUE_PROMPT = (\n",
    "        \"MANY entities and relationships were missed in the last extraction. \"\n",
    "        \"Add them below using the same format:\\n\"\n",
    "    )\n",
    "    LOOP_PROMPT = (\n",
    "        \"It appears some entities and relationships may have still been missed. \"\n",
    "        \"Answer YES | NO if there are still entities or relationships that need to be added.\\n\"\n",
    "    )\n",
    "    elements = []\n",
    "    pbar = tqdm(enumerate(chunks), total=len(chunks))\n",
    "    \n",
    "    for index, chunk in pbar:\n",
    "        pbar.set_description(f\"Processing chunk {index+1} of {len(chunks)}\")\n",
    "        # Initialize the conversation with the initial extraction prompt\n",
    "        current_prompt = extraction_prompt.replace('{input_text}', chunk)\n",
    "        messages = [{\"role\": \"user\", \"content\": current_prompt}]\n",
    "        try:\n",
    "            with torch.no_grad():  # Prevent storing computation graph\n",
    "                response = LLM_engine(messages, max_new_tokens=LLM_setting[\"entity_extraction_length\"], \n",
    "                                      pad_token_id=LLM_setting[\"pad_token_id\"], \n",
    "                                      do_sample = LLM_setting[\"do_sample\"], temperature= LLM_setting[\"temperature\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Error during initial extraction for chunk {index+1}: {e}\")\n",
    "            elements.append(\"\")\n",
    "            continue  # Skip to the next chunk\n",
    "        \n",
    "        # Extract the assistant's reply (adjust according to your LLM's response format)\n",
    "        try:\n",
    "            # Example for OpenAI-like response structure\n",
    "            # entities_and_relations = response['choices'][0]['message']['content'].strip()\n",
    "            # Adjust the above line based on your LLM's actual response format\n",
    "            entities_and_relations = response[-1]['generated_text'][-1]['content'].strip()\n",
    "        except (IndexError, KeyError) as e:\n",
    "            print(f\"Error parsing response for chunk {index+1}: {e}\")\n",
    "            elements.append(\"\")\n",
    "            continue\n",
    "        \n",
    "        results = entities_and_relations\n",
    "        messages.append({\"role\": \"assistant\", \"content\": entities_and_relations})\n",
    "        \n",
    "        # Begin multi-glean checking loop\n",
    "        for gleaning in range(max_gleanings):\n",
    "            try:\n",
    "                # Append CONTINUE_PROMPT to prompt for more entities\n",
    "                messages.append({\"role\": \"user\", \"content\": CONTINUE_PROMPT})\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    response = LLM_engine(messages, max_new_tokens=LLM_setting[\"entity_extraction_length\"], \n",
    "                                      pad_token_id=LLM_setting[\"pad_token_id\"], \n",
    "                                      do_sample = LLM_setting[\"do_sample\"], temperature= LLM_setting[\"temperature\"])\n",
    "                # Extract new entities\n",
    "                new_entities = response[-1]['generated_text'][-1]['content'].strip()\n",
    "                results += '\\n' + new_entities\n",
    "                messages.append({\"role\": \"assistant\", \"content\": new_entities})\n",
    "                \n",
    "                # Check if this is the last iteration\n",
    "                if gleaning >= max_gleanings - 1:\n",
    "                    break\n",
    "                \n",
    "                # Append LOOP_PROMPT to check for remaining entities\n",
    "                messages.append({\"role\": \"user\", \"content\": LOOP_PROMPT})\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    response = LLM_engine(messages, max_new_tokens=LLM_setting[\"entity_condition_length\"], \n",
    "                                      pad_token_id=LLM_setting[\"pad_token_id\"], \n",
    "                                      do_sample = LLM_setting[\"do_sample\"], temperature= LLM_setting[\"temperature\"])               \n",
    "                loop_response = response[-1]['generated_text'][-1]['content'].strip().upper()\n",
    "\n",
    "                if \"YES\" not in loop_response:\n",
    "                    break\n",
    "                # Append the loop response\n",
    "                messages.append({\"role\": \"assistant\", \"content\": loop_response})\n",
    "            except Exception as e:\n",
    "                print(f\"Error during gleaning {gleaning+1} for chunk {index+1}: {e}\")\n",
    "                break  # Exit the gleaning loop on error\n",
    "            \n",
    "            finally:\n",
    "                # Clear CUDA cache after each gleaning to free memory\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # Append the final results for the chunk\n",
    "        elements.append(results)\n",
    "        \n",
    "        # Clear messages list to free memory\n",
    "        del messages\n",
    "        torch.cuda.empty_cache()\n",
    "    return elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "435a84e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "import re\n",
    "from typing import Any, Dict, Tuple, List, Optional\n",
    "from collections.abc import Mapping\n",
    "def clean_str(input: Any) -> str:\n",
    "    \"\"\"Clean an input string by removing HTML escapes, control characters, and other unwanted characters.\"\"\"\n",
    "    # If we get non-string input, just give it back\n",
    "    if not isinstance(input, str):\n",
    "        return input\n",
    "\n",
    "    result = html.unescape(input.strip())\n",
    "    # https://stackoverflow.com/questions/4324790/removing-control-characters-from-a-string-in-python\n",
    "    return re.sub(r\"[\\x00-\\x1f\\x7f-\\x9f]\", \"\", result)\n",
    "\n",
    "\n",
    "def _unpack_descriptions(data: Mapping) -> list[str]:\n",
    "    value = data.get(\"description\", None)\n",
    "    return [] if value is None else value.split(\"\\n\")\n",
    "\n",
    "def _unpack_source_ids(data: Mapping) -> list[str]:\n",
    "    value = data.get(\"source_id\", None)\n",
    "    return [] if value is None else value.split(\", \")\n",
    "\n",
    "def collect_elements_relationship(elements):\n",
    "    graph = nx.Graph()\n",
    "    entitysss = {}\n",
    "    for idx, group in enumerate(elements):\n",
    "        chunk_id = str(idx)\n",
    "        # Specific setting for Qwen2.5\n",
    "        if \"<##\" in group:\n",
    "            entities = group.split('<##')\n",
    "        else:\n",
    "            entities = group.split('##')\n",
    "        entitysss[idx] = []\n",
    "        for entity in entities:\n",
    "            record = entity.strip()\n",
    "            record = re.sub(r\"^\\(|\\)$\", \"\", record.strip())\n",
    "            record = record.replace(\"< | >\", \"<|>\")\n",
    "            record_attributes = record.split('<|>')\n",
    "            if \"entity\" in record_attributes[0] and len(record_attributes) >= 4:\n",
    "                entity_name = clean_str(record_attributes[1].upper())\n",
    "                entitysss[idx].append(entity_name)\n",
    "                \n",
    "                \n",
    "                entity_type = clean_str(record_attributes[2].upper())\n",
    "                entity_description = clean_str(record_attributes[3]) \n",
    "                if entity_name in graph.nodes():\n",
    "                    node = graph.nodes[entity_name]\n",
    "                    node[\"content\"] = \"\\n\".join(\n",
    "                        list({\n",
    "                            *_unpack_descriptions(node),\n",
    "                            entity_description,\n",
    "                        })\n",
    "                    )\n",
    "                    node[\"source_id\"] = \", \".join(\n",
    "                        list({\n",
    "                            *_unpack_source_ids(node),\n",
    "                            str(chunk_id),\n",
    "                        })\n",
    "                    )\n",
    "                    node[\"type\"] = (\n",
    "                        entity_type if entity_type != \"\" else node[\"type\"]\n",
    "                    )\n",
    "                else:\n",
    "                    graph.add_node(\n",
    "                        entity_name,\n",
    "                        type=entity_type,\n",
    "                        content=entity_description,\n",
    "                        source_id=str(chunk_id),\n",
    "                    )\n",
    "            if (\"relationship\" in record_attributes[0] and len(record_attributes) >= 4):\n",
    "                source_node = clean_str(record_attributes[1].upper())\n",
    "                target_node = clean_str(record_attributes[2].upper())\n",
    "                edge_description = clean_str(record_attributes[3])\n",
    "                try:\n",
    "                    weight = float(record_attributes[-1])\n",
    "                except ValueError:\n",
    "                    weight = 1.0\n",
    "                if source_node not in graph.nodes():\n",
    "                    graph.add_node(\n",
    "                        source_node,\n",
    "                        type=\"\",\n",
    "                        content=\"\",\n",
    "                        source_id=chunk_id,\n",
    "                    )\n",
    "                if target_node not in graph.nodes():\n",
    "                    graph.add_node(\n",
    "                        target_node,\n",
    "                        type=\"\",\n",
    "                        content=\"\",\n",
    "                        source_id=chunk_id,\n",
    "                    )\n",
    "                    \n",
    "                if graph.has_edge(source_node, target_node):\n",
    "                    edge_data = graph.get_edge_data(source_node, target_node)\n",
    "                    if edge_data is not None:\n",
    "                        weight += edge_data[\"weight\"]\n",
    "                        edge_description = \"\\n\".join(\n",
    "                            list({\n",
    "                                *_unpack_descriptions(edge_data),\n",
    "                                edge_description,\n",
    "                            })\n",
    "                        )\n",
    "                        chunk_id = \", \".join(\n",
    "                            list({\n",
    "                                *_unpack_source_ids(edge_data),\n",
    "                                str(chunk_id),\n",
    "                            })\n",
    "                        )\n",
    "                graph.add_edge(\n",
    "                    source_node,\n",
    "                    target_node,\n",
    "                    weight=weight,\n",
    "                    content=edge_description,\n",
    "                    source_id=chunk_id,\n",
    "                )                \n",
    "    return graph, entitysss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5e8308d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.metrics import edit_distance\n",
    "from difflib import SequenceMatcher\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Ensure NLTK data is downloaded (only needed once)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def description_similarity(desc1, desc2):\n",
    "    \"\"\"\n",
    "    Computes cosine similarity between two descriptions using TF-IDF vectorization.\n",
    "    Returns a score between 0 and 1.\n",
    "    \"\"\"\n",
    "    # Additional normalization\n",
    "    desc1 = desc1.lower().strip()\n",
    "    desc2 = desc2.lower().strip()\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    vectors = vectorizer.fit_transform([desc1, desc2])\n",
    "    cosine_sim = cosine_similarity(vectors[0:1], vectors[1:2])\n",
    "    return float(cosine_sim[0][0])\n",
    "\n",
    "def get_abbreviation(phrase):\n",
    "    \"\"\"\n",
    "    Generates an abbreviation by taking the first letter of each word in the phrase.\n",
    "    Lemmatizes the result to handle plural forms.\n",
    "    \"\"\"\n",
    "    # Normalize phrase by removing extraneous punctuation and trimming spaces\n",
    "    phrase = phrase.strip()\n",
    "    words = re.split(r'[\\s\\-/]+', phrase)\n",
    "    words = [w for w in words if w]  # remove empty strings\n",
    "    abbreviation = ''.join(word[0].upper() for word in words if word)\n",
    "    abbreviation_lemma = lemmatizer.lemmatize(abbreviation.lower())\n",
    "    return abbreviation_lemma.upper()\n",
    "\n",
    "def normalize_name(name):\n",
    "    \"\"\"\n",
    "    Normalizes an entity name by:\n",
    "    - Lowercasing\n",
    "    - Removing non-alphanumeric characters\n",
    "    - Tokenizing and lemmatizing each token\n",
    "    - Optionally could remove stopwords if desired\n",
    "    \"\"\"\n",
    "    # Remove non-alphanumeric characters\n",
    "    name_clean = re.sub(r'\\W+', ' ', name).lower().strip()\n",
    "    tokens = nltk.word_tokenize(name_clean)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    normalized_name = ' '.join(lemmatized_tokens)\n",
    "    return normalized_name\n",
    "\n",
    "def are_synonyms(word1, word2):\n",
    "    \"\"\"\n",
    "    Checks if two words have overlapping synonym sets from WordNet.\n",
    "    Returns True if synonym sets intersect, False otherwise.\n",
    "    \"\"\"\n",
    "    synsets1 = wn.synsets(word1.lower())\n",
    "    synsets2 = wn.synsets(word2.lower())\n",
    "    if not synsets1 or not synsets2:\n",
    "        return False\n",
    "\n",
    "    # Get all lemmas for word1\n",
    "    lemmas1 = set()\n",
    "    for s in synsets1:\n",
    "        for l in s.lemmas():\n",
    "            lemmas1.add(l.name().lower())\n",
    "\n",
    "    # Get all lemmas for word2\n",
    "    lemmas2 = set()\n",
    "    for s in synsets2:\n",
    "        for l in s.lemmas():\n",
    "            lemmas2.add(l.name().lower())\n",
    "\n",
    "    # Check for intersection\n",
    "    return not lemmas1.isdisjoint(lemmas2)\n",
    "\n",
    "def string_similarity(name1, name2):\n",
    "    \"\"\"\n",
    "    Computes similarity between two strings using:\n",
    "    - Levenshtein-based similarity\n",
    "    - Jaro-Winkler similarity (via difflib)\n",
    "    Returns the average of both similarities.\n",
    "    \"\"\"\n",
    "    name1 = name1.lower().strip()\n",
    "    name2 = name2.lower().strip()\n",
    "\n",
    "    if len(name1) == 0 or len(name2) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Levenshtein Distance Similarity\n",
    "    lev_distance = edit_distance(name1, name2)\n",
    "    lev_similarity = 1 - lev_distance / max(len(name1), len(name2))\n",
    "\n",
    "    # Jaro-Winkler Similarity\n",
    "    jaro_similarity = SequenceMatcher(None, name1, name2).ratio()\n",
    "\n",
    "    avg_similarity = (lev_similarity + jaro_similarity) / 2\n",
    "    return avg_similarity\n",
    "\n",
    "class NodeSimilarity:\n",
    "    def __init__(self):\n",
    "        pass  # No LLM initialization required\n",
    "\n",
    "    def advanced_similarity(self, node1, node2, threshold=0.85):\n",
    "        \"\"\"\n",
    "        Computes an advanced similarity score between two nodes represented as:\n",
    "        node = [entity_name, entity_type, entity_description]\n",
    "\n",
    "        Returns a binary decision (0 or 1) based on whether the computed similarity\n",
    "        surpasses the given threshold. You can also return the continuous similarity\n",
    "        score if desired.\n",
    "        \"\"\"\n",
    "        entity_name1, entity_type1, entity_description1 = node1\n",
    "        entity_name2, entity_type2, entity_description2 = node2\n",
    "\n",
    "        # Normalize entity names\n",
    "        name1_normalized = normalize_name(entity_name1)\n",
    "        name2_normalized = normalize_name(entity_name2)\n",
    "\n",
    "        # Compute abbreviations\n",
    "        abbreviation1 = get_abbreviation(name1_normalized).lower().strip()\n",
    "        abbreviation2 = get_abbreviation(name2_normalized).lower().strip()\n",
    "\n",
    "        # Initial checks for abbreviation matches\n",
    "        # Check if one name is the abbreviation of the other or vice versa\n",
    "        if (name1_normalized == abbreviation2 or name1_normalized == abbreviation2+'s' or\n",
    "            name2_normalized == abbreviation1 or name2_normalized == abbreviation1+'s'):\n",
    "            return 1\n",
    "\n",
    "        # Check synonyms\n",
    "        if are_synonyms(name1_normalized, name2_normalized):\n",
    "            return 1\n",
    "\n",
    "        # Compute name string similarity\n",
    "        name_sim = string_similarity(name1_normalized, name2_normalized)\n",
    "\n",
    "        # Consider entity type: if types differ significantly, reduce effective name_sim\n",
    "        # Example: If they are from completely different domains, we might trust name_sim less\n",
    "        # Simple heuristic: If types are the same, trust name similarity more.\n",
    "        if entity_type1 != entity_type2:\n",
    "            name_sim = name_sim * 0.9  # slightly reduce similarity if types differ\n",
    "        else:\n",
    "            name_sim = name_sim * 1.1  # slightly boost if types are the same, ensure doesn't exceed 1 though\n",
    "            if name_sim > 1:\n",
    "                name_sim = 1\n",
    "\n",
    "        # Description similarity\n",
    "        desc_sim = description_similarity(entity_description1, entity_description2)\n",
    "\n",
    "        # Combine similarities:\n",
    "        # Here we can do a weighted combination. For example:\n",
    "        # Weight name similarity more if entity types match\n",
    "        combined_score = (name_sim * 0.6) + (desc_sim * 0.4)\n",
    "\n",
    "        # If there's a strong name similarity or exact synonyms, we might return early as done above.\n",
    "        # Otherwise, rely on combined score:\n",
    "        if combined_score >= threshold:\n",
    "            return 1\n",
    "\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37765f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "\n",
    "def merge_node_attributes(graph, nodes_to_merge):\n",
    "    \"\"\"\n",
    "    Merges attributes of a group of nodes into a single node attribute dictionary.\n",
    "    Modify this function based on how you want to combine attributes.\n",
    "    \"\"\"\n",
    "    # Example: If all nodes have 'type' and 'content', we can select the most common type,\n",
    "    # and concatenate the content fields.\n",
    "    types = [graph.nodes[n]['type'] for n in nodes_to_merge if 'type' in graph.nodes[n]]\n",
    "    contents = [graph.nodes[n]['content'] for n in nodes_to_merge if 'content' in graph.nodes[n]]\n",
    "\n",
    "    # Resolve 'type' by majority vote or first non-empty (custom logic here)\n",
    "    if types:\n",
    "        from collections import Counter\n",
    "        type_counter = Counter(types)\n",
    "        merged_type = type_counter.most_common(1)[0][0]\n",
    "    else:\n",
    "        merged_type = None\n",
    "\n",
    "    # Concatenate all content (customize as needed)\n",
    "    merged_content = \" \".join(str(c) for c in contents if c)\n",
    "\n",
    "    # You can add more logic for other attributes as needed\n",
    "\n",
    "    merged_attr = {}\n",
    "    if merged_type is not None:\n",
    "        merged_attr['type'] = merged_type\n",
    "    if merged_content:\n",
    "        merged_attr['content'] = merged_content\n",
    "\n",
    "    return merged_attr\n",
    "\n",
    "\n",
    "def clean_graph(graph, similarity_model, SIMILARITY_THRESHOLD=0.85):\n",
    "    # Compute pairwise similarities\n",
    "    node_names = list(graph.nodes())\n",
    "    num_nodes = len(node_names)\n",
    "    similarity_matrix = np.zeros((num_nodes, num_nodes))\n",
    "\n",
    "    for i in tqdm(range(num_nodes)):\n",
    "        for j in range(i + 1, num_nodes):\n",
    "            node_i = [\n",
    "                node_names[i], \n",
    "                graph.nodes[node_names[i]].get('type', ''), \n",
    "                graph.nodes[node_names[i]].get('content', '')\n",
    "            ]\n",
    "            node_j = [\n",
    "                node_names[j], \n",
    "                graph.nodes[node_names[j]].get('type', ''), \n",
    "                graph.nodes[node_names[j]].get('content', '')\n",
    "            ]\n",
    "            if graph.nodes[node_names[j]].get('type', '') != '' and graph.nodes[node_names[i]].get('type', '') != '':\n",
    "                sim_score = similarity_model.advanced_similarity(node_i, node_j, threshold=SIMILARITY_THRESHOLD)\n",
    "            else:\n",
    "                sim_score = 0\n",
    "            similarity_matrix[i, j] = sim_score\n",
    "            similarity_matrix[j, i] = sim_score                \n",
    "\n",
    "    # Identify pairs of nodes above threshold\n",
    "    # We will treat nodes that are pairwise similar as edges in a similarity graph\n",
    "    similarity_graph = nx.Graph()\n",
    "    similarity_graph.add_nodes_from(node_names)\n",
    "\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(i + 1, num_nodes):\n",
    "            if similarity_matrix[i, j] >= SIMILARITY_THRESHOLD:\n",
    "                # Add edge in similarity graph\n",
    "                similarity_graph.add_edge(node_names[i], node_names[j])\n",
    "\n",
    "    # Find connected components of similarity_graph - each component is a group of similar nodes\n",
    "    components = list(nx.connected_components(similarity_graph))\n",
    "\n",
    "    # If a component has only one node, no merge is needed\n",
    "    # If multiple nodes are present, merge them into a single node\n",
    "    # We'll create a new graph with merged nodes\n",
    "    merged_graph = nx.Graph()\n",
    "    # Keep track of old-to-new node mapping\n",
    "    old_to_new = {}\n",
    "\n",
    "    for comp in components:\n",
    "        comp = list(comp)\n",
    "        if len(comp) == 1:\n",
    "            # Just copy the node as is\n",
    "            n = comp[0]\n",
    "            merged_graph.add_node(n, **graph.nodes[n])\n",
    "            old_to_new[n] = n\n",
    "        else:\n",
    "            # Merge all nodes in the component into a single node\n",
    "            merged_attr = merge_node_attributes(graph, comp)\n",
    "\n",
    "            # Create a new representative node name - for example:\n",
    "            # You could use a join of node names or pick the first node name\n",
    "            # Here, let's pick the first node as representative\n",
    "            representative = comp[0]\n",
    "            merged_graph.add_node(representative, **merged_attr)\n",
    "\n",
    "            # Update mapping\n",
    "            for n in comp:\n",
    "                old_to_new[n] = representative\n",
    "\n",
    "    # Now, add edges to merged_graph:\n",
    "    # For every original edge (u,v,data) in the old graph,\n",
    "    # map u and v to their new representatives and add an edge if it doesn't exist.\n",
    "    # If attributes need merging, handle similarly. For now, we assume no attribute conflict.\n",
    "    for u, v, data in graph.edges(data=True):\n",
    "        new_u = old_to_new[u]\n",
    "        new_v = old_to_new[v]\n",
    "        if new_u != new_v:\n",
    "            # Combine edge attributes if needed, for simplicity just add if not present\n",
    "            # If the edge already exists, you may want to merge attributes\n",
    "            if merged_graph.has_edge(new_u, new_v):\n",
    "                # If you need to merge edge attributes, do so here\n",
    "                pass\n",
    "            else:\n",
    "                merged_graph.add_edge(new_u, new_v, **data)\n",
    "\n",
    "    return merged_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb66f267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "from pyvis.network import Network\n",
    "from IPython.display import IFrame, display\n",
    "\n",
    "def visualize(graph, filename='graph2.html'):\n",
    "    # Step 1: Extract unique node types from the graph\n",
    "    unique_types = set()\n",
    "    for node, data in graph.nodes(data=True):\n",
    "        node_type = data.get('type', 'default')\n",
    "        unique_types.add(node_type)\n",
    "\n",
    "    # Step 2: Assign colors to node types\n",
    "    num_types = len(unique_types)\n",
    "    cmap = cm.get_cmap('hsv', num_types)\n",
    "    color_list = [mcolors.rgb2hex(cmap(i)) for i in range(cmap.N)]\n",
    "    type_color_map = dict(zip(sorted(unique_types), color_list))\n",
    "\n",
    "    # Determine if graph is directed\n",
    "    directed = graph.is_directed()\n",
    "\n",
    "    # Step 3: Create a pyvis Network\n",
    "    net = Network(height='750px', width='100%', notebook=True, directed=directed)\n",
    "\n",
    "    # Add nodes\n",
    "    for node, data in graph.nodes(data=True):\n",
    "        node_type = data.get('type', 'default')\n",
    "        color = type_color_map.get(node_type, 'gray')\n",
    "        net.add_node(\n",
    "            node, \n",
    "            label=node, \n",
    "            title=data.get('content', ''), \n",
    "            color=color,\n",
    "            **data\n",
    "        )\n",
    "\n",
    "    # Add edges\n",
    "    for u, v, edata in graph.edges(data=True):\n",
    "        # You can pass edge attributes as needed\n",
    "        net.add_edge(u, v, **edata)\n",
    "\n",
    "    # Customize the appearance\n",
    "    net.repulsion(node_distance=200, central_gravity=0.3)\n",
    "    net.toggle_physics(True)\n",
    "\n",
    "    # Generate and display the network\n",
    "    net.show(filename)\n",
    "\n",
    "    # Display in Jupyter Notebook (if in such an environment)\n",
    "    display(IFrame(filename, width='100%', height='750px'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76c40eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def save_meta_info(meta_info, file_path='meta_info.json'):\n",
    "    \"\"\"\n",
    "    Save the meta_info dictionary to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        meta_info (dict): The meta information to save.\n",
    "        file_path (str): The path to the JSON file where data will be saved.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure the directory exists\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "        \n",
    "        # Open the file in write mode with UTF-8 encoding\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            # Serialize and write the dictionary to the file with indentation for readability\n",
    "            json.dump(meta_info, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        print(f\"meta_info successfully saved to {file_path}\")\n",
    "    \n",
    "    except TypeError as te:\n",
    "        print(\"Serialization Error: Ensure all data in meta_info is JSON-serializable.\")\n",
    "        print(te)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"An unexpected error occurred while saving meta_info:\")\n",
    "        print(e)\n",
    "        \n",
    "    \n",
    "def load_meta_info(file_path='meta_info.json'):\n",
    "    \"\"\"\n",
    "    Load the meta_info dictionary from a JSON file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the JSON file to load.\n",
    "\n",
    "    Returns:\n",
    "        dict: The loaded meta_info dictionary.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            meta_info = json.load(f)\n",
    "        print(f\"meta_info successfully loaded from {file_path}\")\n",
    "        return meta_info\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {file_path} does not exist.\")\n",
    "    \n",
    "    except json.JSONDecodeError as jde:\n",
    "        print(\"Error decoding JSON. Ensure the file is properly formatted.\")\n",
    "        print(jde)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"An unexpected error occurred while loading meta_info:\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1594cbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def load_and_process_documents(folder_path):\n",
    "    \"\"\"Load documents and split them into sentence-aware chunks.\"\"\"\n",
    "    documents = load_document(folder_path)\n",
    "    chunks = split_documents_into_chunks(documents)\n",
    "    return documents, chunks\n",
    "\n",
    "def extract_meta_info(LLM, folder_path, domain_name='Source Code Vulnerability'):\n",
    "    \"\"\"Extract meta information from documents.\"\"\"\n",
    "    llm_sim_model = NodeSimilarity()\n",
    "    extraction_prompt = load_prompt(EXTRACTION_PROMPT_PATH)\n",
    "    processed_extraction_prompt = process_entity_extraction_prompt(extraction_prompt)\n",
    "    type_extraction_prompt = load_prompt(TYPE_EXTRACTION_PROMPT_PATH)\n",
    "    documents, chunks = load_and_process_documents(folder_path)\n",
    "\n",
    "    # Extract entity types\n",
    "    # Need more advanced techniques !!\n",
    "    entity_type = entity_type_extraction(\n",
    "        LLM,\n",
    "        type_extraction_prompt,\n",
    "        document='\\n'.join(random.sample(chunks, 3)),\n",
    "        domain_name=domain_name\n",
    "    )\n",
    "    # Update extraction prompt with entity types\n",
    "    extraction_prompt = extraction_prompt.replace('{entity_types}', entity_type)\n",
    "    \n",
    "    print(\"Start elements extraction\")\n",
    "    # Extract elements from chunks\n",
    "    elements = extract_elements_from_chunks(LLM, chunks, processed_extraction_prompt)\n",
    "    # Collect relationships\n",
    "    graph, _ = collect_elements_relationship(elements)\n",
    "    \n",
    "    # Clean the graph\n",
    "    updated_graph = clean_graph(graph, llm_sim_model)\n",
    "    \n",
    "    # Compile meta information\n",
    "    meta_info = { \n",
    "        \"entity_type\": entity_type,       # string\n",
    "        \"elements\": elements,             # list\n",
    "     }\n",
    "    return meta_info, graph, updated_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72b85667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')\n",
    "\n",
    "def load_indexing(folder_path, LLM, CHECKPOINT_PATH):\n",
    "    # Ensure the checkpoint directory exists\n",
    "    checkpoint_dir = CHECKPOINT_PATH.parent\n",
    "    if not checkpoint_dir.exists():\n",
    "        checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    try:\n",
    "        if CHECKPOINT_PATH.exists():\n",
    "            # Load existing meta information and graphs\n",
    "            logging.info(f\"Checkpoint found at {CHECKPOINT_PATH}, loading existing meta information.\")\n",
    "            meta_info = load_meta_info(file_path=str(CHECKPOINT_PATH))\n",
    "            \n",
    "            org_graph_path = checkpoint_dir / 'org_graph.graphml'\n",
    "            updated_graph_path = checkpoint_dir / 'update_graph.graphml'\n",
    "            \n",
    "            if not org_graph_path.exists() or not updated_graph_path.exists():\n",
    "                raise FileNotFoundError(\"One or both of the graph checkpoint files are missing.\")\n",
    "            \n",
    "            org_graph = nx.read_graphml(str(org_graph_path))\n",
    "            updated_graph = nx.read_graphml(str(updated_graph_path))\n",
    "            logging.info(\"Successfully loaded meta information and graphs from checkpoint.\")\n",
    "        else:\n",
    "            # Extract and save new meta information if checkpoint doesn't exist\n",
    "            logging.info(\"No checkpoint found. Extracting meta information from the provided folder and LLM.\")\n",
    "            meta_info, org_graph, updated_graph = extract_meta_info(LLM, folder_path)\n",
    "            \n",
    "            # Save the extracted information\n",
    "            save_meta_info(meta_info, file_path=str(CHECKPOINT_PATH))\n",
    "            nx.write_graphml(org_graph, str(checkpoint_dir / 'org_graph.graphml'))\n",
    "            nx.write_graphml(updated_graph, str(checkpoint_dir / 'update_graph.graphml'))\n",
    "            logging.info(\"Extracted and saved meta information and graphs to checkpoint.\")\n",
    "        \n",
    "        return meta_info, org_graph, updated_graph\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred while loading or generating the meta information: {e}\", exc_info=True)\n",
    "        # Optionally, you can re-raise or return None\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "730a1c74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import html\n",
    "\n",
    "\n",
    "def find_main_and_subgraphs(G: nx.Graph) -> Tuple[nx.Graph, List[nx.Graph]]:\n",
    "    \"\"\"识别主图和与主图无关的子图。\"\"\"\n",
    "    # 获取所有连通组件，按大小降序排列\n",
    "    connected_components = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "    \n",
    "    if not connected_components:\n",
    "        return G, []\n",
    "    \n",
    "    # 主图是最大的连通组件\n",
    "    main_component = connected_components[0]\n",
    "    main_graph = G.subgraph(main_component).copy()\n",
    "    \n",
    "    # 其他子图\n",
    "    subgraphs = [G.subgraph(c).copy() for c in connected_components[1:]]\n",
    "    \n",
    "    return main_graph, subgraphs\n",
    "\n",
    "def run_leiden(\n",
    "    graph: nx.Graph, args: Dict[str, Any]\n",
    ") -> Tuple[Dict[int, Dict[int, List[str]]], Dict[int, int]]:\n",
    "    \"\"\"Run Leiden community detection on the given graph.\"\"\"\n",
    "    max_cluster_size = args.get(\"max_cluster_size\", 10)\n",
    "    use_lcc = args.get(\"use_lcc\", True)\n",
    "    node_id_to_community_map, community_hierarchy_map = _compute_leiden_communities(\n",
    "        graph=graph,\n",
    "        max_cluster_size=max_cluster_size,\n",
    "        use_lcc=use_lcc,\n",
    "        seed=args.get(\"seed\", 0xDEADBEEF),\n",
    "    )\n",
    "    levels = args.get(\"levels\")\n",
    "\n",
    "    # If they don't pass in levels, use them all\n",
    "    if levels is None:\n",
    "        levels = sorted(node_id_to_community_map.keys())\n",
    "\n",
    "    results_by_level: Dict[int, Dict[int, List[str]]] = {}\n",
    "    for level in levels:\n",
    "        result = {}\n",
    "        results_by_level[level] = result\n",
    "        for node_id, raw_community_id in node_id_to_community_map[level].items():\n",
    "            community_id = raw_community_id\n",
    "            if community_id not in result:\n",
    "                result[community_id] = []\n",
    "            result[community_id].append(node_id)\n",
    "    return results_by_level, community_hierarchy_map\n",
    "\n",
    "\n",
    "def _compute_leiden_communities(\n",
    "    graph: nx.Graph,\n",
    "    max_cluster_size: int,\n",
    "    use_lcc: bool,\n",
    "    seed=0xDEADBEEF,\n",
    ") -> Tuple[Dict[int, Dict[str, int]], Dict[int, int]]:\n",
    "    \"\"\"Compute Leiden communities using hierarchical_leiden from graspologic.\"\"\"\n",
    "    from graspologic.partition import hierarchical_leiden\n",
    "\n",
    "    if use_lcc:\n",
    "        graph = stable_largest_connected_component(graph)\n",
    "    \n",
    "\n",
    "    community_mapping = hierarchical_leiden(\n",
    "        graph, max_cluster_size=max_cluster_size, random_seed=seed\n",
    "    )\n",
    "\n",
    "    results: Dict[int, Dict[str, int]] = {}\n",
    "    hierarchy: Dict[int, int] = {}\n",
    "    for partition in community_mapping:\n",
    "        results.setdefault(partition.level, {})\n",
    "        results[partition.level][partition.node] = partition.cluster\n",
    "        hierarchy[partition.cluster] = (\n",
    "            partition.parent_cluster if partition.parent_cluster is not None else -1\n",
    "        )\n",
    "    return results, hierarchy\n",
    "\n",
    "\n",
    "def stable_largest_connected_component(graph: nx.Graph) -> nx.Graph:\n",
    "    \"\"\"Return the largest connected component of the graph in a stable manner.\"\"\"\n",
    "    from graspologic.utils import largest_connected_component\n",
    "\n",
    "    graph = graph.copy()\n",
    "    graph = largest_connected_component(graph)\n",
    "#     graph = normalize_node_names(graph)\n",
    "    return _stabilize_graph(graph)\n",
    "\n",
    "\n",
    "def normalize_node_names(graph: nx.Graph | nx.DiGraph) -> nx.Graph | nx.DiGraph:\n",
    "    \"\"\"Normalize node names.\"\"\"\n",
    "    node_mapping = {node: html.unescape(node.upper().strip()) for node in graph.nodes()}  # type: ignore\n",
    "    return nx.relabel_nodes(graph, node_mapping)\n",
    "\n",
    "\n",
    "def _stabilize_graph(graph: nx.Graph) -> nx.Graph:\n",
    "    \"\"\"Ensure an undirected graph with the same relationships will always be read the same way.\"\"\"\n",
    "    fixed_graph = nx.DiGraph() if graph.is_directed() else nx.Graph()\n",
    "\n",
    "    sorted_nodes = graph.nodes(data=True)\n",
    "    sorted_nodes = sorted(sorted_nodes, key=lambda x: x[0])\n",
    "\n",
    "    fixed_graph.add_nodes_from(sorted_nodes)\n",
    "    edges = list(graph.edges(data=True))\n",
    "    # If the graph is undirected, we create the edges in a stable way, so we get the same results\n",
    "    # for example:\n",
    "    # A -> B\n",
    "    # in graph theory is the same as\n",
    "    # B -> A\n",
    "    # in an undirected graph\n",
    "    # however, this can lead to downstream issues because sometimes\n",
    "    # consumers read graph.nodes() which ends up being [A, B] and sometimes it's [B, A]\n",
    "    # but they base some of their logic on the order of the nodes, so the order ends up being important\n",
    "    # so we sort the nodes in the edge in a stable way, so that we always get the same order\n",
    "    if not graph.is_directed():\n",
    "\n",
    "        def _sort_source_target(edge):\n",
    "            source, target, edge_data = edge\n",
    "            if source > target:\n",
    "                temp = source\n",
    "                source = target\n",
    "                target = temp\n",
    "            return source, target, edge_data\n",
    "\n",
    "        edges = [_sort_source_target(edge) for edge in edges]\n",
    "\n",
    "    def _get_edge_key(source: Any, target: Any) -> str:\n",
    "        return f\"{source} -> {target}\"\n",
    "\n",
    "    edges = sorted(edges, key=lambda x: _get_edge_key(x[0], x[1]))\n",
    "\n",
    "    fixed_graph.add_edges_from(edges)\n",
    "    return fixed_graph\n",
    "\n",
    "def subgraph_collecting(results_by_level, gtype = 'major'):\n",
    "    graph_dic = {}\n",
    "    if gtype == 'major':\n",
    "        for main_key in results_by_level:\n",
    "            for sec_key in results_by_level[main_key]:\n",
    "                graph_dic[sec_key] = results_by_level[main_key][sec_key]\n",
    "                \n",
    "    else:\n",
    "        for idx, sg in enumerate(results_by_level):\n",
    "            graph_dic[idx] = list(sg.nodes())\n",
    "    return graph_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d6f2465",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_node_description(nodes_list, G):\n",
    "    \"\"\"\n",
    "    Generate descriptive strings for nodes and edges based on a given subgraph defined by nodes_list.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nodes_list : list\n",
    "        A list of nodes for which we want to generate descriptions.\n",
    "    G : networkx.Graph\n",
    "        The full graph or a subgraph containing the nodes and edges. Nodes are expected to have a\n",
    "        'content' attribute, and edges are expected to have a 'description' attribute.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    node_descriptions : str\n",
    "        A string containing descriptions of each node in `nodes_list`.\n",
    "    edge_descriptions : str\n",
    "        A string containing descriptions of each edge between the nodes in `nodes_list`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build node descriptions based on node attributes\n",
    "    node_descriptions = \"\\n\\n\".join(\n",
    "        f\"{node}: {G.nodes[node].get('content', '')}\" \n",
    "        for node in nodes_list\n",
    "    ) + \"\\n\\n\"\n",
    "\n",
    "    # Identify valid neighbors (within nodes_list) and build edge descriptions\n",
    "    edge_set = set()\n",
    "    edge_lines = []\n",
    "\n",
    "    # Iterate over each node in nodes_list to find edges within this induced subgraph\n",
    "    for node in nodes_list:\n",
    "        for neighbor in G.neighbors(node):\n",
    "            if neighbor in nodes_list:\n",
    "                \n",
    "                # Sort pair to handle undirected graphs (avoid duplicate edges)\n",
    "                # If the graph is directed, you can skip sorting or adjust logic accordingly\n",
    "                edge_nodes = tuple(sorted([node, neighbor]))\n",
    "                \n",
    "                if edge_nodes not in edge_set:\n",
    "                    edge_set.add(edge_nodes)\n",
    "\n",
    "                    # Attempt to retrieve the edge description from G\n",
    "                    # For undirected graphs, G[node][neighbor] == G[neighbor][node]\n",
    "                    edge_description = G[node][neighbor].get('content', '')\n",
    "                    \n",
    "                    # Only add an entry if there's a description\n",
    "                    if edge_description:\n",
    "                        edge_key = f\"{node}<|>{neighbor}\"\n",
    "                        edge_lines.append(f\"{edge_key}: {edge_description}\")\n",
    "\n",
    "    edge_descriptions = \"\\n\\n\".join(edge_lines).strip()\n",
    "    return node_descriptions.strip(), edge_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85d168a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_answer(LLM, cur_testcase, do_sample=True, temperature = 0.5, max_new_tokens=20, pad_token_id=None):\n",
    "    if not pad_token_id:\n",
    "        pad_token_id = LLM.tokenizer.eos_token_id\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": cur_testcase},\n",
    "    ]\n",
    "    outputs = LLM(\n",
    "        messages,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample = do_sample,\n",
    "        temperature = temperature,\n",
    "        pad_token_id=pad_token_id\n",
    "    )\n",
    "    res = outputs[0][\"generated_text\"][-1]\n",
    "    torch.cuda.empty_cache()\n",
    "    return res['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "373c2824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_leaf_nodes(community_hierarchy_map):\n",
    "    # All nodes in the hierarchy\n",
    "    all_nodes = set(community_hierarchy_map.keys())\n",
    "    \n",
    "    # All nodes that are parents (excluding -1)\n",
    "    parent_nodes = set(parent for parent in community_hierarchy_map.values() if parent != -1)\n",
    "    \n",
    "    # Leaf nodes are those that are not parents of any node\n",
    "    leaf_nodes = all_nodes - parent_nodes\n",
    "    return leaf_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a58df3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt import graph_merging_finestage\n",
    "def extract_val(text):\n",
    "    if text.isdigit():\n",
    "        return float(text)\n",
    "    else:\n",
    "        try:\n",
    "            numbers = re.findall(r'\\d+\\.?\\d*', text) \n",
    "            numbers = [float(num) if '.' in num else int(num) for num in numbers]\n",
    "            return numbers[0]\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    \n",
    "def subgraph_matching(major_community, minor_community, prompt, LLM, G):\n",
    "    LLM_engine, LLM_setting = LLM\n",
    "    evaluation_form = {}\n",
    "    for skey, min_graph in tqdm(minor_community.items()):\n",
    "        evaluation_form[skey] = {}\n",
    "        min_nodes_des, min_edges_des = generate_node_description(min_graph, G)\n",
    "        for tkey, maj_graph in major_community.items():\n",
    "            maj_nodes_des, maj_edges_des = generate_node_description(maj_graph, G)\n",
    "            cur_prompt = prompt.format(snodes_info=min_nodes_des, sedges_info=min_nodes_des,\n",
    "                                      tnodes_info=maj_nodes_des, tedges_info=maj_edges_des)\n",
    "            \n",
    "            \n",
    "            res = agent_answer(LLM_engine, cur_prompt, \n",
    "                               do_sample=LLM_setting[\"do_sample\"], \n",
    "                               max_new_tokens=LLM_setting[\"subgraph_matching_length\"],\n",
    "                                temperature=LLM_setting[\"temperature\"])\n",
    "\n",
    "            evaluation_form[skey][tkey] = res\n",
    "    return evaluation_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc6dffb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt import summarize_descriptions\n",
    "def update_entity_description(updated_graph, prompt, LLM):\n",
    "    LLM_engine, LLM_setting = LLM\n",
    "    graph = updated_graph.copy()\n",
    "    nodes_list = graph.nodes()\n",
    "    for node in tqdm(nodes_list):\n",
    "        node_info = graph.nodes[node]\n",
    "        cur_content = node_info[\"content\"]\n",
    "        if cur_content:\n",
    "            cur_prompt = prompt.format(description_list=cur_content, entity_name=node)\n",
    "            updated_decription = agent_answer(LLM_engine, cur_prompt, \n",
    "                                  do_sample=LLM_setting[\"do_sample\"], max_new_tokens=LLM_setting[\"update_entity_description\"],\n",
    "                                  temperature=LLM_setting[\"temperature\"])\n",
    "            node_info[\"content\"] = updated_decription\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e214228",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda\\envs\\CRA_LLM\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import networkx as nx\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def merge_discrete_nodes_into_major_graph(updated_graph, LLM):\n",
    "    \"\"\"\n",
    "    如果本地已有 `merge_results.json` 则直接读取，不再重新进行计算和保存；\n",
    "    否则正常进行计算、保存并返回结果。\n",
    "\n",
    "    Parameters:\n",
    "    - updated_graph (networkx.Graph): 待合并的图对象\n",
    "    - LLM: 用于子图匹配的语言模型\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary 包含:\n",
    "        \"concat_graph\": 合并后的图\n",
    "        \"non_relevant_keys\": 未被合并的键列表\n",
    "        \"G2G_finematch\": 小社区到大社区匹配结果\n",
    "        \"major_community\": 大社区结构\n",
    "        \"minor_community\": 小社区结构\n",
    "    \"\"\"\n",
    "\n",
    "    # 如果本地有结果文件，直接读取返回\n",
    "    json_path = 'checkpoint/merge_results.json'\n",
    "    graph_path = 'checkpoint/concat_graph.graphml'\n",
    "    if os.path.exists(json_path) and os.path.exists(graph_path):\n",
    "        with open(json_path, 'r') as f:\n",
    "            loaded_data = json.load(f)\n",
    "        # 重新读取图\n",
    "        concat_graph = nx.read_graphml(graph_path)\n",
    "        return {\n",
    "            \"concat_graph\": concat_graph,\n",
    "            \"non_relevant_keys\": loaded_data.get(\"non_relevant_keys\", []),\n",
    "            \"G2G_finematch\": loaded_data.get(\"G2G_finematch\", {}),\n",
    "            \"major_community\": loaded_data.get(\"major_community\", {}),\n",
    "            \"minor_community\": loaded_data.get(\"minor_community\", {})\n",
    "        }\n",
    "    \n",
    "    # update graph description\n",
    "    print(\"Start Updating the Description\")\n",
    "    updated_graph = update_entity_description(updated_graph, prompt=summarize_descriptions.SUMMARIZE_PROMPT, LLM=LLM)\n",
    "    NLI_pipe = pipeline(\"text-classification\", model=\"sileod/deberta-v3-base-tasksource-nli\")\n",
    "    main_graph, minor_subgraphs = find_main_and_subgraphs(updated_graph)\n",
    "    max_group = max(len(min_graph.nodes()) for min_graph in minor_subgraphs)\n",
    "    seg_args = {\n",
    "        \"max_cluster_size\": max_group,\n",
    "        \"use_lcc\": True,\n",
    "        \"seed\": 0xDEADBEEF\n",
    "    }\n",
    "\n",
    "    results_by_level, community_hierarchy_map = run_leiden(updated_graph, seg_args)\n",
    "    leaf_nodes = get_leaf_nodes(community_hierarchy_map)\n",
    "    major_community = subgraph_collecting(results_by_level, gtype='major')\n",
    "    major_community_leaf = {key: major_community[key] for key in leaf_nodes}\n",
    "    minor_community = subgraph_collecting(minor_subgraphs, gtype='minor')\n",
    "    print(\"Start Graph Corase-Stage Merging\")\n",
    "    merge_prompt = graph_merging_finestage.Graph_Merging_PROMPT_FINE\n",
    "    graph_merging_res = subgraph_matching(\n",
    "        major_community_leaf, \n",
    "        minor_community, \n",
    "        merge_prompt, \n",
    "        LLM, \n",
    "        updated_graph\n",
    "    )\n",
    "\n",
    "    G2G_finematch = {}\n",
    "    for mkey, score_list in graph_merging_res.items():\n",
    "        filter_score = {key: extract_val(score) for key, score in score_list.items()}\n",
    "        max_value = max(filter_score.values())\n",
    "        max_index = [key for key, value in filter_score.items() if value == max_value]\n",
    "        G2G_finematch[mkey] = max_index\n",
    "\n",
    "    non_relevant_keys = []\n",
    "    concat_graph = updated_graph.copy()\n",
    "    print(\"Start Graph Fine-Stage Merging\")\n",
    "    for min_key, min_nodes in tqdm(minor_community.items(), desc=\"Merging Nodes\"):\n",
    "        related_major_group = G2G_finematch.get(min_key, [])\n",
    "        maj_nodes = [leaf for key in related_major_group for leaf in major_community_leaf.get(key, [])]\n",
    "        marker = 0\n",
    "        \n",
    "        for snode in min_nodes:\n",
    "            for tnode in maj_nodes:\n",
    "                snode_info = updated_graph.nodes[snode].get('content')\n",
    "                tnode_info = updated_graph.nodes[tnode].get('content')\n",
    "                \n",
    "                if snode_info and tnode_info:\n",
    "                    label_forward = NLI_pipe([{\"text\": snode_info, \"text_pair\": tnode_info}])[0]\n",
    "                    label_backward = NLI_pipe([{\"text\": tnode_info, \"text_pair\": snode_info}])[0]\n",
    "                    flabel = label_forward.get('label', '')\n",
    "                    blabel = label_backward.get('label', '')\n",
    "\n",
    "                    if 'entailment' in flabel.lower() or 'entailment' in blabel.lower():\n",
    "                        marker = 1\n",
    "                        concat_graph.add_edge(\n",
    "                            snode,\n",
    "                            tnode,\n",
    "                            weight=1,\n",
    "                            content=\"High relevant edges based on hypothesis\",\n",
    "                            source_id=\"\"\n",
    "                        )\n",
    "        if marker == 0:\n",
    "            non_relevant_keys.append(min_key)\n",
    "\n",
    "    nx.write_graphml(concat_graph, graph_path)\n",
    "    \n",
    "    result_dict = {\n",
    "        \"concat_graph\": concat_graph,\n",
    "        \"non_relevant_keys\": non_relevant_keys,\n",
    "        \"G2G_finematch\": G2G_finematch,\n",
    "        \"major_community\": major_community,\n",
    "        \"minor_community\": minor_community\n",
    "    }\n",
    "\n",
    "    serializable_dict = {\n",
    "        \"non_relevant_keys\": non_relevant_keys,\n",
    "        \"G2G_finematch\": G2G_finematch,\n",
    "        \"major_community\": major_community,\n",
    "        \"minor_community\": minor_community\n",
    "    }\n",
    "\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(serializable_dict, f, indent=4)\n",
    "\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4b09c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def obtain_node_edge_df(G):\n",
    "    nodes_list = list(G.nodes())\n",
    "    node_to_id = {node: i for i, node in enumerate(nodes_list)}\n",
    "\n",
    "    # 2. Create node DataFrame with unique numerical IDs\n",
    "    nodes_data = []\n",
    "    for node in nodes_list:\n",
    "        data = G.nodes[node]\n",
    "        nodes_data.append({\n",
    "            \"node_id\": node_to_id[node],\n",
    "            \"node_name\": node,\n",
    "            \"type\": data.get(\"type\", None),\n",
    "            \"content\": data.get(\"content\", None)\n",
    "        })\n",
    "\n",
    "    nodes_df = pd.DataFrame(nodes_data)\n",
    "\n",
    "    # 3. Create edge DataFrame with unique numerical IDs\n",
    "    edges_data = []\n",
    "    for edge_id, (u, v, data) in enumerate(G.edges(data=True)):\n",
    "        edges_data.append({\n",
    "            \"edge_id\": edge_id,\n",
    "            \"source_node_id\": node_to_id[u],\n",
    "            \"target_node_id\": node_to_id[v],\n",
    "            \"source_node\": u,\n",
    "            \"target_node\": v,\n",
    "            \"weight\": data.get(\"weight\", None),\n",
    "            \"content\": data.get(\"content\", None),\n",
    "            \"source_id\": data.get(\"source_id\", None)\n",
    "        })\n",
    "\n",
    "    edges_df = pd.DataFrame(edges_data)\n",
    "    return nodes_df, edges_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1ddb7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_node_info(node_name, df):\n",
    "    info = df.loc[df[\"node_name\"]==node_name].iloc[0].to_dict()\n",
    "    info['content'] = info['content'].replace(\"Here is a comprehensive summary of the data:\\n\\n\", \"\")\n",
    "    return info\n",
    "    \n",
    "def obtain_edge_info(source: any, target: any, df: pd.DataFrame):\n",
    "    required_columns = {'source_node', 'target_node'}\n",
    "    if not required_columns.issubset(df.columns):\n",
    "        raise KeyError(f\"DataFrame must include: {required_columns}\")\n",
    "\n",
    "    if source == target:\n",
    "        filtered_df = df[(df['source_node'] == source) & (df['target_node'] == target)]\n",
    "    else:\n",
    "        filtered_df = df[\n",
    "            ((df['source_node'] == source) & (df['target_node'] == target)) |\n",
    "            ((df['source_node'] == target) & (df['target_node'] == source))\n",
    "        ]\n",
    "    if not filtered_df.empty:\n",
    "        result = filtered_df.to_dict(orient='records')\n",
    "        return result[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def estimiate_token_length(sequence_info, LLM):\n",
    "    sequence_info = [str(i) for i in sequence_info]\n",
    "    overall_info = ','.join(sequence_info)\n",
    "    length = len(LLM.tokenizer.encode(overall_info, add_special_tokens=False))\n",
    "    return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9cf67d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_leaf_report(G, community_dict, leaf_nodes, length_limit, community_report_dict, prompt, LLM, nodes_df, edges_df):\n",
    "    \"\"\"\n",
    "    Generate a report for each leaf community. This function:\n",
    "    1. Extracts subgraphs corresponding to leaf communities.\n",
    "    2. Prioritizes edges by sum of degrees of their endpoints.\n",
    "    3. Iteratively adds node and edge info until the length limit is reached.\n",
    "    4. Sends the compiled information to an LLM to generate a community report.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    G : networkx.Graph\n",
    "        The main graph.\n",
    "    community_dict : dict\n",
    "        A dictionary mapping community identifiers to the list of nodes in that community.\n",
    "    leaf_nodes : list\n",
    "        A list of leaf community identifiers.\n",
    "    length_limit : int\n",
    "        Maximum allowed token length for the context.\n",
    "    community_report_dict : dict\n",
    "        Dictionary to store the resulting reports.\n",
    "    prompt : str\n",
    "        Prompt template string, which will be formatted with the input text before sending to LLM.\n",
    "    LLM : object\n",
    "        Language model or API for generating text.\n",
    "    nodes_df : pd.DataFrame\n",
    "        DataFrame containing node information.\n",
    "    edges_df : pd.DataFrame\n",
    "        DataFrame containing edge information.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Updated community_report_dict with generated reports for the leaf communities.\n",
    "    \"\"\"\n",
    "    LLM_engine, LLM_setting = LLM\n",
    "    for community in tqdm(leaf_nodes):\n",
    "        \n",
    "        cur_nodes = community_dict[community]\n",
    "        subG = G.subgraph(cur_nodes)\n",
    "\n",
    "        # Build priority list based on node degrees\n",
    "        priority_list = {}\n",
    "        for source, target in subG.edges:\n",
    "            edge_weight = G.degree(source) + G.degree(target)\n",
    "            priority_list[(source, target)] = edge_weight\n",
    "\n",
    "        # Sort edges by priority (descending)\n",
    "        sorted_priority_list = sorted(priority_list.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        context_size = 0\n",
    "\n",
    "        cur_node_info = []\n",
    "        cur_edge_info = []\n",
    "        existing_node_ids = set()\n",
    "        existing_edge_ids = set()\n",
    "\n",
    "        # Iterate over edges in priority order, adding nodes and edges if token length allows\n",
    "        for edge, _ in sorted_priority_list:\n",
    "            source, target = edge\n",
    "\n",
    "            # Add source node info if not already added\n",
    "            if context_size <= length_limit and source not in existing_node_ids:\n",
    "                source_info = obtain_node_info(source, nodes_df)\n",
    "                key_info = [source_info['node_id'], source_info['node_name'], source_info['content']]\n",
    "                cur_len = estimiate_token_length(key_info, LLM_engine)\n",
    "                if context_size + cur_len <= length_limit:\n",
    "                    context_size += cur_len\n",
    "                    cur_node_info.append(source_info)\n",
    "                    existing_node_ids.add(source)\n",
    "\n",
    "            # Add target node info if not already added\n",
    "            if context_size <= length_limit and target not in existing_node_ids:\n",
    "                target_info = obtain_node_info(target, nodes_df)\n",
    "                key_info = [target_info['node_id'], target_info['node_name'], target_info['content']]\n",
    "                cur_len = estimiate_token_length(key_info, LLM_engine)\n",
    "                if context_size + cur_len <= length_limit:\n",
    "                    context_size += cur_len\n",
    "                    cur_node_info.append(target_info)\n",
    "                    existing_node_ids.add(target)\n",
    "\n",
    "            # Add edge info if not already added\n",
    "            if context_size <= length_limit and edge not in existing_edge_ids:\n",
    "                edge_info = obtain_edge_info(source, target, edges_df)\n",
    "                key_info = [edge_info['edge_id'], edge_info['source_node'], edge_info['target_node'], edge_info['content']]\n",
    "                cur_len = estimiate_token_length(key_info, LLM_engine)\n",
    "                if context_size + cur_len <= length_limit:\n",
    "                    context_size += cur_len\n",
    "                    cur_edge_info.append(edge_info)\n",
    "                    existing_edge_ids.add(edge)\n",
    "\n",
    "        # Convert gathered info into CSV-like strings\n",
    "        tmp_node_df = pd.DataFrame(cur_node_info)[[\"node_id\", \"node_name\", \"content\"]]\n",
    "        tmp_node_df = tmp_node_df.rename(columns={'node_id': 'id', 'node_name': 'entity', 'content': 'description'})\n",
    "        node_content = tmp_node_df.to_csv(index=False, sep=',', encoding='utf-8').replace('\"', '')\n",
    "\n",
    "        tmp_edge_df = pd.DataFrame(cur_edge_info)[[\"edge_id\", \"source_node\", \"target_node\", \"content\"]]\n",
    "        tmp_edge_df = tmp_edge_df.rename(columns={\"edge_id\": \"id\", \"source_node\": \"source\",\n",
    "                                                  \"target_node\": \"target\", \"content\": \"description\"})\n",
    "        edge_content = tmp_edge_df.to_csv(index=False, sep=',', encoding='utf-8').replace('\"', '')\n",
    "\n",
    "        # Prepare input data for the LLM\n",
    "        input_data = f\"Entities\\n{node_content}Relationships\\n{edge_content}\"\n",
    "\n",
    "        # Generate the community report using the LLM\n",
    "        report_text = agent_answer(LLM_engine, prompt.format(input_text=input_data), \n",
    "                                   do_sample= LLM_setting[\"do_sample\"], max_new_tokens=LLM_setting[\"community_sum_length\"],\n",
    "                                  temperature=LLM_setting[\"temperature\"])\n",
    "\n",
    "        # Store the results in the community_report_dict\n",
    "        community_report_dict[community] = {\n",
    "            'node_info': node_content,\n",
    "            'edge_info': edge_content,\n",
    "            'len_info': len(node_content) + len(edge_content),\n",
    "            'community_report': report_text,\n",
    "            'community_report_len': estimiate_token_length([report_text], LLM_engine)\n",
    "        }\n",
    "\n",
    "    return community_report_dict\n",
    "\n",
    "\n",
    "def generate_parent_report(G, community_dict, children_map, length_limit, community_report_dict, prompt, LLM):\n",
    "    \"\"\"\n",
    "    Generate a report for parent communities based on their child communities' reports.\n",
    "    If the aggregated length exceeds the limit, some child community reports are replaced\n",
    "    by their summarized forms.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    G : networkx.Graph\n",
    "        The main graph.\n",
    "    community_dict : dict\n",
    "        Dictionary mapping community IDs to node lists.\n",
    "    children_map : dict\n",
    "        Dictionary mapping a parent community to its child communities.\n",
    "    length_limit : int\n",
    "        Maximum allowed token length.\n",
    "    community_report_dict : dict\n",
    "        Dictionary containing previously generated community reports for child communities.\n",
    "    prompt : str\n",
    "        Prompt template for LLM that will be formatted with the input data.\n",
    "    LLM : object\n",
    "        The language model or API used for generating text.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Updated community_report_dict with parent community reports.\n",
    "    \"\"\"\n",
    "\n",
    "    # Process parents from bottom-up\n",
    "    LLM_engine, LLM_setting = LLM\n",
    "    for community in tqdm(list(children_map.keys())[::-1]):\n",
    "        child_communities = children_map[community]\n",
    "\n",
    "        # If no child communities, skip processing\n",
    "        if not child_communities:\n",
    "            continue\n",
    "\n",
    "        # Extract the reports for all child communities\n",
    "        parent_info = {key: community_report_dict[key] for key in child_communities if key in community_report_dict}\n",
    "\n",
    "        # Calculate the total length from all children\n",
    "        overall_len_info = {key: parent_info[key]['len_info'] for key in parent_info}\n",
    "        cur_length = sum(overall_len_info.values())\n",
    "\n",
    "        # Lists to hold final node and edge CSV contents\n",
    "        content_node_list = []\n",
    "        content_edge_list = []\n",
    "        # List to hold summarized (shortened) community reports when length exceeds limit\n",
    "        pre_report_list = []\n",
    "\n",
    "        if cur_length > length_limit:\n",
    "            # Sort children by length of their content in descending order\n",
    "            sorted_children = sorted(overall_len_info.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "            # Identify children to convert to summaries until we fall below the length limit\n",
    "            convert_keys = []\n",
    "            iter_idx = 0\n",
    "            while cur_length > length_limit and iter_idx < len(sorted_children):\n",
    "                cur_key = sorted_children[iter_idx][0]\n",
    "                # Add current key to conversion list\n",
    "                convert_keys.append(cur_key)\n",
    "                # Calculate new length after \"converting\" current key to a summary\n",
    "                # Converted keys contribute only their report length (since we'll use their summary instead of node/edge info)\n",
    "                converted_len = sum(parent_info[k]['community_report_len'] for k in convert_keys)\n",
    "                remaining_keys = [k for k in overall_len_info.keys() if k not in convert_keys]\n",
    "                remaining_len = sum(overall_len_info[k] for k in remaining_keys)\n",
    "                cur_length = converted_len + remaining_len\n",
    "                iter_idx += 1\n",
    "            # Now that some keys might be converted:\n",
    "            original_keys = [k for k in overall_len_info.keys() if k not in convert_keys]\n",
    "\n",
    "            # Add summaries of converted keys (replace their node/edge details with their report text)\n",
    "            for key in convert_keys:\n",
    "                pre_report_list.append(parent_info[key]['community_report'])\n",
    "\n",
    "            # Add the node and edge content of the original (non-converted) keys\n",
    "            for key in original_keys:\n",
    "                content_node_list.append(parent_info[key]['node_info'])\n",
    "                content_edge_list.append(parent_info[key]['edge_info'])\n",
    "        else:\n",
    "            # If length is already within the limit, use all child info as-is\n",
    "            for key in parent_info:\n",
    "                content_node_list.append(parent_info[key]['node_info'])\n",
    "                content_edge_list.append(parent_info[key]['edge_info'])\n",
    "\n",
    "        def merge_csv_contents(csv_list):\n",
    "            \"\"\"\n",
    "            Merge multiple CSV strings into one, removing duplicate headers.\n",
    "            Assumes the first line of each CSV is a header line.\n",
    "            \"\"\"\n",
    "            final_content = []\n",
    "            for idx, csv_content in enumerate(csv_list):\n",
    "                lines = csv_content.strip().split('\\n')\n",
    "                # Keep header only from the first CSV\n",
    "                if idx == 0:\n",
    "                    final_content.extend(lines)\n",
    "                else:\n",
    "                    # Skip the header line for subsequent CSVs\n",
    "                    if len(lines) > 1:\n",
    "                        final_content.extend(lines[1:])\n",
    "                    # If only a header line was present, ignore it\n",
    "            return \"\\n\".join(final_content)\n",
    "\n",
    "        content_node_final = merge_csv_contents(content_node_list)\n",
    "        content_edge_final = merge_csv_contents(content_edge_list)\n",
    "\n",
    "        # Prepare input data for LLM\n",
    "        if pre_report_list:\n",
    "            # If there are summaries, prepend them as \"Relevant Community\"\n",
    "            community_content = \"\\n\".join(pre_report_list)\n",
    "            input_data = (\n",
    "                f\"Relevant Community\\n{community_content}\\n\"\n",
    "                f\"Entities\\n{content_node_final}\\n\"\n",
    "                f\"Relationships\\n{content_edge_final}\"\n",
    "            )\n",
    "        else:\n",
    "            input_data = f\"Entities\\n{content_node_final}\\nRelationships\\n{content_edge_final}\"\n",
    "\n",
    "        # Generate the parent community report\n",
    "        report_text = agent_answer(LLM_engine, prompt.format(input_text=input_data), \n",
    "                                   do_sample= LLM_setting[\"do_sample\"], max_new_tokens=LLM_setting[\"community_sum_length\"],\n",
    "                                  temperature=LLM_setting[\"temperature\"])\n",
    "        if pre_report_list:\n",
    "            community_report_dict[community] = {\n",
    "                'node_info': content_node_final,\n",
    "                'edge_info': content_edge_final,\n",
    "                'len_info': len(content_node_final) + len(content_edge_final),\n",
    "                'contained_community':community_content,\n",
    "                'community_report': report_text,\n",
    "                'community_report_len': estimiate_token_length([report_text], LLM_engine)\n",
    "            }            \n",
    "            \n",
    "        else:\n",
    "            # Store the parent community report\n",
    "            community_report_dict[community] = {\n",
    "                'node_info': content_node_final,\n",
    "                'edge_info': content_edge_final,\n",
    "                'len_info': len(content_node_final) + len(content_edge_final),\n",
    "                'community_report': report_text,\n",
    "                'community_report_len': estimiate_token_length([report_text], LLM_engine)\n",
    "            }\n",
    "\n",
    "    return community_report_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc91fc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import logging\n",
    "from prompt import community_report\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def generate_community_report(\n",
    "    concat_info: Dict[str, Any],\n",
    "    results_by_level: Dict[Any, Dict[Any, Any]],\n",
    "    community_hierarchy_map: Dict[Any, Any],\n",
    "    nodes_df: pd.DataFrame,\n",
    "    edges_df: pd.DataFrame,\n",
    "    LLM: Any,\n",
    "    LLM_args: Dict[str, Any],\n",
    "    length_limit: int\n",
    ") -> Dict[Any, Any]:\n",
    "    \"\"\"\n",
    "    Generate a comprehensive community report based on hierarchical community data.\n",
    "\n",
    "    Args:\n",
    "        concat_info (Dict[str, Any]): Contains concatenated graph information.\n",
    "        results_by_level (Dict[Any, Dict[Any, Any]]): Community results organized by levels.\n",
    "        community_hierarchy_map (Dict[Any, Any]): Mapping of child to parent community IDs.\n",
    "        nodes_df (Any): DataFrame containing node information.\n",
    "        edges_df (Any): DataFrame containing edge information.\n",
    "        LLM (Any): Language model instance for report generation.\n",
    "        LLM_args (Dict[str, Any]): Arguments for the language model.\n",
    "        length_limit (int): Maximum length for the generated reports.\n",
    "\n",
    "    Returns:\n",
    "        Dict[Any, Any]: Dictionary containing reports for each community.\n",
    "    \"\"\"\n",
    "    \n",
    "    def is_leaf(node: Any, children_map: Dict[Any, list]) -> bool:\n",
    "        return not children_map.get(node)\n",
    "\n",
    "    try:\n",
    "        # Build community_dict from results_by_level\n",
    "        community_dict = {\n",
    "            community_id: details\n",
    "            for level in results_by_level\n",
    "            for community_id, details in results_by_level[level].items()\n",
    "        }\n",
    "        logger.debug(f\"Constructed community_dict with {len(community_dict)} communities.\")\n",
    "\n",
    "        # Build children_map and identify root nodes\n",
    "        children_map = defaultdict(list)\n",
    "        root_nodes = []\n",
    "        for child, parent in community_hierarchy_map.items():\n",
    "            if parent == -1:\n",
    "                root_nodes.append(child)\n",
    "            else:\n",
    "                children_map[parent].append(child)\n",
    "        logger.debug(f\"Identified {len(root_nodes)} root nodes.\")\n",
    "\n",
    "        # Identify all leaf communities\n",
    "        leaf_nodes = [node for node in community_hierarchy_map if is_leaf(node, children_map)]\n",
    "        logger.info(f\"Found {len(leaf_nodes)} leaf nodes.\")\n",
    "\n",
    "        # Extract the main graph\n",
    "        G = concat_info.get('concat_graph')\n",
    "        if G is None:\n",
    "            logger.error(\"concat_graph not found in concat_info.\")\n",
    "            raise ValueError(\"concat_graph is required in concat_info.\")\n",
    "\n",
    "        # Initialize community_report_dict\n",
    "        community_report_dict = {}\n",
    "\n",
    "        # Generate reports for leaf communities\n",
    "        logger.info(\"Generating leaf communities report.\")\n",
    "        community_report_dict = generate_leaf_report(\n",
    "            G=G,\n",
    "            community_dict=community_dict,\n",
    "            leaf_nodes=leaf_nodes,\n",
    "            length_limit=length_limit,\n",
    "            community_report_dict=community_report_dict,\n",
    "            prompt=community_report.COMMUNITY_REPORT_PROMPT,\n",
    "            LLM=(LLM, LLM_args),\n",
    "            nodes_df=nodes_df,\n",
    "            edges_df=edges_df\n",
    "        )\n",
    "\n",
    "        # Generate reports for parent communities\n",
    "        logger.info(\"Generating parent communities report.\")\n",
    "        community_report_dict = generate_parent_report(\n",
    "            G=G, \n",
    "            community_dict=community_dict, \n",
    "            children_map=children_map, \n",
    "            length_limit=length_limit, \n",
    "            community_report_dict=community_report_dict,\n",
    "            prompt=community_report.COMMUNITY_REPORT_PROMPT,\n",
    "            LLM=(LLM, LLM_args)\n",
    "        )\n",
    "\n",
    "        logger.info(\"Community report generation completed successfully.\")\n",
    "        return community_report_dict\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(\"An error occurred while generating the community report.\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9730d893",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_community(\n",
    "    path: str = 'checkpoint',\n",
    "    concat_info: Dict[str, Any]= None,\n",
    "    nodes_df: Any = None,          \n",
    "    edges_df: Any = None,          \n",
    "    llm_config: Optional[Dict[str, Any]] = None,\n",
    "    seg_args: Optional[Dict[str, Any]] = None,\n",
    "    meta_info_filename: str = 'community_meta_info.json',\n",
    "    length_limit: int = 1000        \n",
    ") -> Dict[str, Any]:\n",
    "    \n",
    "    LLM = llm_config.get(\"engine\")\n",
    "    LLM_args = llm_config.get(\"settings\", {})    \n",
    "    \"\"\"\n",
    "    Load community meta information from a file if it exists; otherwise, perform community segmentation,\n",
    "    generate reports, and save the meta information.\n",
    "\n",
    "    Args:\n",
    "        path (str, optional): Directory path to check for the meta info file. Defaults to 'checkpoint'.\n",
    "        concat_info (Dict[str, Any]): Contains concatenated graph information.\n",
    "        results_by_level (Optional[Dict[Any, Dict[Any, Any]]], optional): \n",
    "            Community results organized by levels. Defaults to None.\n",
    "        community_hierarchy_map (Optional[Dict[Any, Any]], optional): \n",
    "            Mapping of child to parent community IDs. Defaults to None.\n",
    "        nodes_df (Any, optional): DataFrame containing node information. Defaults to None.\n",
    "        edges_df (Any, optional): DataFrame containing edge information. Defaults to None.\n",
    "        LLM (Any, optional): Language model instance for report generation. Defaults to None.\n",
    "        LLM_args (Optional[Dict[str, Any]], optional): Arguments for the language model. Defaults to None.\n",
    "        seg_args (Optional[Dict[str, Any]], optional): Arguments for the Leiden algorithm. Defaults to None.\n",
    "        meta_info_filename (str, optional): Filename for the meta info. Defaults to 'meta_info.json'.\n",
    "        length_limit (int, optional): Maximum length for the generated reports. Defaults to 1000.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: Dictionary containing community meta information.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        meta_info_file = os.path.join(path, meta_info_filename)\n",
    "        logger.debug(f\"Constructed meta_info_file path: {meta_info_file}\")\n",
    "\n",
    "        if os.path.exists(meta_info_file):\n",
    "            logger.info(f\"Loading community meta info from {meta_info_file}.\")\n",
    "            community_meta_info = load_meta_info(file_path=meta_info_file)\n",
    "            logger.info(\"Community meta info loaded successfully.\")\n",
    "        else:\n",
    "            logger.info(\"Meta info file not found. Running community segmentation using Leiden algorithm.\")\n",
    "            \n",
    "            # Validate presence of 'concat_graph' in concat_info\n",
    "            if 'concat_graph' not in concat_info:\n",
    "                logger.error(\"'concat_graph' key is missing from concat_info.\")\n",
    "                raise KeyError(\"'concat_graph' must be present in concat_info.\")\n",
    "            \n",
    "            # Run Leiden algorithm to get community results\n",
    "            results_by_level, community_hierarchy_map = run_leiden(\n",
    "                concat_info['concat_graph'], \n",
    "                seg_args\n",
    "            )\n",
    "            logger.info(\"Community segmentation completed successfully.\")\n",
    "\n",
    "            # Generate community reports\n",
    "            logger.info(\"Generating community reports.\")\n",
    "            community_report_dict = generate_community_report(\n",
    "                concat_info=concat_info,\n",
    "                results_by_level=results_by_level,\n",
    "                community_hierarchy_map=community_hierarchy_map,\n",
    "                nodes_df=nodes_df,\n",
    "                edges_df=edges_df,\n",
    "                LLM=LLM,\n",
    "                LLM_args=LLM_args,\n",
    "                length_limit=length_limit\n",
    "            )\n",
    "            logger.info(\"Community reports generated successfully.\")\n",
    "\n",
    "            # Compile meta information\n",
    "            community_meta_info = {\n",
    "                \"results_by_level\": results_by_level, \n",
    "                \"community_hierarchy_map\": community_hierarchy_map,\n",
    "                \"community_report_dict\": community_report_dict,\n",
    "            }\n",
    "            logger.debug(f\"Compiled community_meta_info: {community_meta_info.keys()}\")\n",
    "\n",
    "            # Ensure the directory exists\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            logger.debug(f\"Ensured the directory '{path}' exists.\")\n",
    "\n",
    "            logger.info(f\"Saving community meta info to {meta_info_file}.\")\n",
    "            save_meta_info(community_meta_info, file_path=meta_info_file)\n",
    "            logger.info(\"Community meta info saved successfully.\")\n",
    "\n",
    "        return community_meta_info\n",
    "\n",
    "    except FileNotFoundError as fnf_error:\n",
    "        logger.exception(f\"File not found error: {fnf_error}\")\n",
    "        raise\n",
    "    except KeyError as key_error:\n",
    "        logger.exception(f\"Key error: {key_error}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.exception(\"An unexpected error occurred while loading or generating community meta information.\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2548f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Tuple, Dict, Any\n",
    "from prompt import missing_prediction\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def flat_dict(target_dict: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Flattens a dictionary into a string with each key-value pair on a new line.\n",
    "\n",
    "    Args:\n",
    "        target_dict (Dict[str, Any]): The dictionary to flatten.\n",
    "\n",
    "    Returns:\n",
    "        str: The flattened string.\n",
    "    \"\"\"\n",
    "    return \"\\n\".join(f\"{key}: {value}\" for key, value in target_dict.items())\n",
    "\n",
    "\n",
    "def obtain_neighbor_info(\n",
    "    graph: nx.Graph, \n",
    "    node: str, \n",
    "    max_neighbor: Optional[int] = 8\n",
    ") -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Retrieves and formats neighbor node information and edge information for a given node.\n",
    "\n",
    "    Args:\n",
    "        graph: The graph data structure.\n",
    "        node (str): The node identifier.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, str]: A tuple containing flattened neighbor node information and edge information.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        neighbors = list(graph.neighbors(node))\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error retrieving neighbors for node '{node}': {e}\")\n",
    "        return \"\", \"\"\n",
    "\n",
    "    if max_neighbor is not None:\n",
    "        # Sort neighbors by degree in descending order and select top `max_neighbor`\n",
    "        neighbors_sorted = sorted(neighbors, key=lambda n: graph.degree(n), reverse=True)\n",
    "        selected_neighbors = neighbors_sorted[:max_neighbor]\n",
    "        neighbors = selected_neighbors\n",
    "        \n",
    "    neighbor_info = {}\n",
    "    edge_info = {}\n",
    "    for nei_node in neighbors:\n",
    "        neighbor_info[nei_node] = graph.nodes[nei_node]\n",
    "        edge_data = graph.get_edge_data(node, nei_node)\n",
    "        if edge_data and 'content' in edge_data:\n",
    "            edge_info[f\"{node}-{nei_node}\"] = edge_data['content']\n",
    "        else:\n",
    "            logger.warning(f\"Missing 'content' for edge '{node}-{nei_node}'.\")\n",
    "            edge_info[f\"{node}-{nei_node}\"] = \"No content available.\"\n",
    "\n",
    "    return flat_dict(neighbor_info), flat_dict(edge_info)\n",
    "\n",
    "\n",
    "def fill_missing_node(graph, all_node_types: str, llm_config: Dict[str, Any]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Fills in missing node types and content by generating them using an LLM.\n",
    "\n",
    "    Args:\n",
    "        graph: The graph data structure.\n",
    "        all_node_types (str): A string listing all possible node types.\n",
    "        llm_config (Dict[str, Any]): Configuration for the LLM, including engine and settings.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, str]: A dictionary mapping node identifiers to their filled content.\n",
    "    \"\"\"\n",
    "    llm_engine = llm_config.get(\"engine\")\n",
    "    llm_settings = llm_config.get(\"settings\", {})\n",
    "    filled_nodes = {}\n",
    "    overall_missing_nodes = [node for node in graph.nodes() if graph.nodes[node].get('content') == \"\"]\n",
    "    sidx = 1\n",
    "    for node in graph.nodes():\n",
    "        node_info = graph.nodes[node]\n",
    "        node_type = node_info.get('type')\n",
    "        node_content = node_info.get('content')\n",
    "\n",
    "        if not node_type or not node_content:\n",
    "            neighbor_nodes_info, neighbor_edges_info = obtain_neighbor_info(graph, node)\n",
    "\n",
    "            try:\n",
    "                prompt = missing_prediction.NODE_MISSING_PROMPT.format(\n",
    "                    cur_node=node,\n",
    "                    neighbor_Node=neighbor_nodes_info,\n",
    "                    edge_info=neighbor_edges_info,\n",
    "                    all_node_type=all_node_types\n",
    "                )\n",
    "            except KeyError as e:\n",
    "                logger.error(f\"Missing placeholder in NODE_MISSING_PROMPT: {e}\")\n",
    "                continue\n",
    "\n",
    "            entity_text = agent_answer(\n",
    "                LLM=llm_engine,\n",
    "                cur_testcase=prompt,\n",
    "                do_sample=llm_settings.get(\"do_sample\", False),\n",
    "                max_new_tokens=llm_settings.get(\"max_new_tokens\", 150),\n",
    "                temperature=llm_settings.get(\"temperature\", 0)\n",
    "            )\n",
    "            if entity_text:\n",
    "                filled_nodes[node] = entity_text\n",
    "                logger.info(f\"{sidx}/{len(overall_missing_nodes)}:Filled node '{node}' with content.\")\n",
    "                sidx += 1\n",
    "            else:\n",
    "                logger.warning(f\"No content returned for node '{node}'.\")\n",
    "\n",
    "    return filled_nodes\n",
    "\n",
    "\n",
    "def fill_missing_edge(graph, llm_config: Dict[str, Any]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Fills in missing edge descriptions by generating them using an LLM.\n",
    "\n",
    "    Args:\n",
    "        graph: The graph data structure.\n",
    "        llm_config (Dict[str, Any]): Configuration for the LLM, including engine and settings.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, str]: A dictionary mapping edge identifiers to their filled content.\n",
    "    \"\"\"\n",
    "    llm_engine = llm_config.get(\"engine\")\n",
    "    llm_settings = llm_config.get(\"settings\", {})\n",
    "    filled_edges = {}\n",
    "    sidx = 1\n",
    "    overall_missing_edges = [edge for edge in graph.edges() if graph.get_edge_data(edge[0], edge[1]).get('content') == \"High relevant edges based on hypothesis\"]\n",
    "    \n",
    "    for edge in graph.edges():\n",
    "        source, target = edge\n",
    "        edge_data = graph.get_edge_data(source, target)\n",
    "\n",
    "        # Check if the edge content indicates it needs to be filled\n",
    "        if edge_data.get('content') == \"High relevant edges based on hypothesis\":\n",
    "            source_node = graph.nodes[source]\n",
    "            target_node = graph.nodes[target]\n",
    "            source_nodes_info, source_edges_info = obtain_neighbor_info(graph, source, max_neighbor=llm_settings[\"filling_max_neighbor\"])\n",
    "            target_nodes_info, target_edges_info = obtain_neighbor_info(graph, target, max_neighbor=llm_settings[\"filling_max_neighbor\"])\n",
    "            try:\n",
    "                prompt = missing_prediction.EDGE_MISSING_PROMPT.format(\n",
    "                    source=source,\n",
    "                    target=target,\n",
    "                    source_info=source_node,\n",
    "                    target_info=target_node,\n",
    "                    source_neighbor_info=source_nodes_info,\n",
    "                    source_edge_info=source_edges_info,\n",
    "                    target_neighbor_info=target_nodes_info,\n",
    "                    target_edge_info=target_edges_info\n",
    "                )\n",
    "            except KeyError as e:\n",
    "                logger.error(f\"Missing placeholder in EDGE_MISSING_PROMPT: {e}\")\n",
    "                continue\n",
    "\n",
    "            edge_text = agent_answer(\n",
    "                LLM=llm_engine,\n",
    "                cur_testcase=prompt,\n",
    "                do_sample=llm_settings.get(\"do_sample\", False),\n",
    "                max_new_tokens=llm_settings.get(\"max_new_tokens\", 150),\n",
    "                temperature=llm_settings.get(\"temperature\", 0)\n",
    "            )\n",
    "            if edge_text:\n",
    "                # need to improve this spliter\n",
    "                filled_edges[f\"{source}-{target}\"] = edge_text\n",
    "                logger.info(f\"{sidx}/{len(overall_missing_edges)}:Filled edge '{source}-{target}' with content.\")\n",
    "                sidx+=1\n",
    "            else:\n",
    "                logger.warning(f\"No content returned for edge '{source}-{target}'.\")\n",
    "\n",
    "    return filled_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee64a2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts JSON content from a given text string and validates required fields.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text containing JSON.\n",
    "\n",
    "    Returns:\n",
    "        dict: The extracted JSON as a dictionary if valid, else empty dict.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Regex to find JSON within code blocks\n",
    "        code_block_pattern = r\"```json\\s*(\\{.*?\\})\\s*```\"\n",
    "        match = re.search(code_block_pattern, text, re.DOTALL)\n",
    "        \n",
    "        if match:\n",
    "            json_str = match.group(1)\n",
    "        else:\n",
    "            # If no code block, attempt to find JSON directly\n",
    "            json_start = text.find('{')\n",
    "            json_end = text.rfind('}')\n",
    "            if json_start == -1 or json_end == -1:\n",
    "                raise ValueError(\"No JSON object found in the response.\")\n",
    "            json_str = text[json_start:json_end+1]\n",
    "        \n",
    "        # Parse the JSON string\n",
    "        data = json.loads(json_str)\n",
    "        \n",
    "        # Validate required fields\n",
    "        required_fields = ['Type', 'Content'] if 'Type' in json_str else ['Content']\n",
    "        for field in required_fields:\n",
    "            if field not in data:\n",
    "                logger.warning(f\"Missing '{field}' in the extracted JSON.\")\n",
    "                data[field] = \"N/A\"  # or handle as needed\n",
    "        \n",
    "        return data\n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.error(f\"JSON decoding failed: {e}\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting JSON: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "acb41570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_graph(\n",
    "    graph: nx.Graph,\n",
    "    llm_config: Dict[str, Any],\n",
    "    folder: str = \"checkpoint\",\n",
    "    save_path: str = \"missing_meta_info.json\"\n",
    ") -> nx.Graph:\n",
    "    \"\"\"\n",
    "    Fills missing node and edge information in the graph using an LLM and caches the results.\n",
    "\n",
    "    Args:\n",
    "        graph (nx.Graph): The original graph.\n",
    "        llm_config (Dict[str, Any]): Configuration for the LLM, including engine and settings.\n",
    "        folder (str, optional): Directory to save/load the meta information. Defaults to \"checkpoint\".\n",
    "        save_path (str, optional): Filename for the meta information. Defaults to \"missing_meta_info.json\".\n",
    "\n",
    "    Returns:\n",
    "        nx.Graph: The updated graph with missing node and edge information filled.\n",
    "    \"\"\"\n",
    "    # Create a copy of the graph to avoid mutating the original\n",
    "    updated_graph = graph.copy()\n",
    "    \n",
    "    # Ensure the folder exists\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    \n",
    "    # Build the full file path\n",
    "    loc = os.path.join(folder, save_path)\n",
    "    \n",
    "    # Initialize meta_info\n",
    "    meta_info = {}\n",
    "    \n",
    "    if os.path.exists(loc):\n",
    "        meta_info = load_meta_info(file_path=loc)\n",
    "    else:\n",
    "        # Fill missing nodes and edges\n",
    "        filled_node_info = fill_missing_node(\n",
    "            graph=updated_graph,\n",
    "            all_node_types=llm_config.get('entity_type', ''),\n",
    "            llm_config=llm_config\n",
    "        )\n",
    "        filled_edge_info = fill_missing_edge(\n",
    "            graph=updated_graph,\n",
    "            llm_config=llm_config\n",
    "        )\n",
    "        meta_info = {\n",
    "            'missing_node': filled_node_info,\n",
    "            'missing_edge': filled_edge_info\n",
    "        }\n",
    "        save_meta_info(meta_info, file_path=loc)\n",
    "\n",
    "\n",
    "    # Extract filled node and edge information from meta_info\n",
    "    filled_node_info = meta_info.get('missing_node', {})\n",
    "    filled_edge_info = meta_info.get('missing_edge', {})\n",
    "    \n",
    "    # Update nodes with filled information\n",
    "    for node, node_json in filled_node_info.items():\n",
    "        node_content = extract_json(node_json)\n",
    "        if node_content:\n",
    "            types = node_content.get('Type')\n",
    "            content = node_content.get('Content')\n",
    "            if types:\n",
    "                updated_graph.nodes[node]['type'] = types\n",
    "            if content:\n",
    "                updated_graph.nodes[node]['content'] = content\n",
    "        else:\n",
    "            # Fallback if JSON extraction failed\n",
    "            updated_graph.nodes[node]['content'] = filled_node_info[node]\n",
    "            logger.warning(f\"Failed to extract JSON for node '{node}'. Updated 'content' with raw data.\")\n",
    "    \n",
    "    # Update edges with filled information\n",
    "    for edge_key, edge_json in filled_edge_info.items():\n",
    "\n",
    "        source, target = (edge_key.split('\"-\"')[0], edge_key.split('\"-\"')[1])\n",
    "        source += '\"'\n",
    "        target = '\"' + target\n",
    "        if updated_graph.has_edge(source, target):\n",
    "            edge_content = extract_json(edge_json)\n",
    "            if edge_content and 'Content' in edge_content:\n",
    "                \n",
    "                updated_graph.edges[source, target]['content'] = edge_content['Content']\n",
    "            else:\n",
    "                updated_graph.edges[source, target]['content'] = edge_json\n",
    "                logger.warning(f\"Failed to extract 'Content' for edge '{source}-{target}'. Updated 'content' with raw data.\")\n",
    "        else:\n",
    "            logger.warning(f\"Edge '{source}-{target}' does not exist in the graph.\")\n",
    "\n",
    "    return updated_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "644694b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def locate_community(level_dict, node_name):\n",
    "    for cid in level_dict:\n",
    "        if node_name in level_dict[cid]:\n",
    "            return cid\n",
    "\n",
    "def load_community_info(level_info, nodes_list):\n",
    "    layer_com = []\n",
    "    for node_name in nodes_list:\n",
    "        cid = locate_community(level_info, node_name)\n",
    "        if cid:\n",
    "            layer_com.append(cid)\n",
    "        else:\n",
    "            layer_com.append('None')\n",
    "    return layer_com\n",
    "\n",
    "def extract_community_json(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts JSON content from a given text string and validates required fields.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text containing JSON.\n",
    "\n",
    "    Returns:\n",
    "        dict: The extracted JSON as a dictionary if valid, else empty dict.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Regex to find JSON within code blocks\n",
    "        code_block_pattern = r\"```json\\s*(\\{.*?\\})\\s*```\"\n",
    "        match = re.search(code_block_pattern, text, re.DOTALL)\n",
    "        \n",
    "        if match:\n",
    "            json_str = match.group(1)\n",
    "        else:\n",
    "            # If no code block, attempt to find JSON directly\n",
    "            json_start = text.find('{')\n",
    "            json_end = text.rfind('}')\n",
    "            if json_start == -1 or json_end == -1:\n",
    "                raise ValueError(\"No JSON object found in the response.\")\n",
    "            json_str = text[json_start:json_end+1]\n",
    "        \n",
    "        # Parse the JSON string\n",
    "        data = json.loads(json_str)\n",
    "        return data\n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.error(f\"JSON decoding failed: {e}\")\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting JSON: {e}\")\n",
    "        return text\n",
    "    \n",
    "    \n",
    "def check_level(key, community_levels):\n",
    "    for level in community_levels:\n",
    "        if key in community_levels[level]:\n",
    "            return level\n",
    "        \n",
    "def process_communities(nodes_df, community_meta_info):\n",
    "    \"\"\"\n",
    "    Processes community information and updates the nodes DataFrame with community levels.\n",
    "    Also creates a DataFrame containing community details.\n",
    "\n",
    "    Parameters:\n",
    "    - nodes_df (pd.DataFrame): DataFrame containing node information with a 'node_name' column.\n",
    "    - community_meta_info (dict): Dictionary containing community metadata.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Updated nodes_df and community_df DataFrames.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract node names as a list for processing\n",
    "    nodes_list = nodes_df[\"node_name\"].tolist()\n",
    "\n",
    "    # Define the community levels to process (e.g., Level 1 and Level 2)\n",
    "    community_levels = community_meta_info.get('results_by_level', {})\n",
    "    \n",
    "    # Iterate over each level to load and assign community information\n",
    "    for level_str, level_data in community_levels.items():\n",
    "        try:\n",
    "            # Convert level to integer for naming (e.g., '0' -> 1)\n",
    "            level_int = int(level_str) + 1\n",
    "            community_column = f'Community_Level_{level_int}'\n",
    "            \n",
    "            # Load community information using the provided function\n",
    "            community_info = load_community_info(level_data, nodes_list)\n",
    "            # Assign the community information to the corresponding column in nodes_df\n",
    "            nodes_df[community_column] = community_info\n",
    "        except (ValueError, KeyError) as e:\n",
    "            # Handle potential errors in level conversion or data loading\n",
    "            print(f\"Error processing level {level_str}: {e}\")\n",
    "            nodes_df[community_column] = None  # Assign NaN or a default value\n",
    "            \n",
    "\n",
    "    # Process community keys sorted numerically\n",
    "    community_report_dict = community_meta_info.get('community_report_dict', {})\n",
    "    community_hierarchy_map = community_meta_info.get('community_hierarchy_map', {})\n",
    "    \n",
    "    # Sort community keys as integers\n",
    "    try:\n",
    "        sorted_keys = sorted(community_report_dict.keys(), key=lambda k: int(k))\n",
    "    except ValueError as e:\n",
    "        print(f\"Error sorting community keys: {e}\")\n",
    "        sorted_keys = list(community_report_dict.keys())  # Fallback to unsorted keys\n",
    "\n",
    "    # Prepare data for the community DataFrame\n",
    "    community_data = {\n",
    "        \"community_key\": [],\n",
    "        \"content\": [],\n",
    "        \"parent\": [],\n",
    "        \"level\":[]\n",
    "    }\n",
    "\n",
    "    for key in sorted_keys:\n",
    "        try:\n",
    "            # Convert key to integer\n",
    "            key_int = int(key)\n",
    "            cur_level = check_level(key, community_levels)\n",
    "            # Extract and process community report\n",
    "            community_report = community_report_dict[key].get('community_report', {})\n",
    "            content = extract_community_json(community_report)\n",
    "            \n",
    "            # Extract parent information from hierarchy map\n",
    "            parent = community_hierarchy_map.get(key, None)\n",
    "            \n",
    "            # Append data to the lists\n",
    "            community_data[\"community_key\"].append(key_int)\n",
    "            # have parsing error in query stage, therefore using org form\n",
    "            community_data[\"content\"].append(community_report)\n",
    "            community_data[\"parent\"].append(parent)\n",
    "            community_data[\"level\"].append(cur_level)\n",
    "        except (ValueError, KeyError, TypeError) as e:\n",
    "            # Handle missing or malformed data\n",
    "            print(f\"Error processing community key {key}: {e}\")\n",
    "            community_data[\"community_key\"].append(None)\n",
    "            community_data[\"content\"].append(None)\n",
    "            community_data[\"parent\"].append(None)\n",
    "\n",
    "    # Create the community DataFrame\n",
    "    community_df = pd.DataFrame(community_data)\n",
    "\n",
    "    return nodes_df, community_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b17f775",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hughj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "2024-12-23 19:42:09,609 [INFO] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:12<00:00,  3.15s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline\n",
    "import nltk\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import re\n",
    "import random\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "LLM = pipeline(\"text-generation\", \"meta-llama/Meta-Llama-3-8B-Instruct\", torch_dtype=torch.bfloat16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "772e6843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the missing edge/node?\n",
    "# Missing node and edge description\n",
    "\n",
    "# leidan settings \n",
    "seg_args = {\n",
    "    \"max_cluster_size\": 10,\n",
    "    \"use_lcc\": True,\n",
    "    \"seed\": 0xDEADBEEF\n",
    "}\n",
    "# Set a length limit for input token context.\n",
    "length_limit = 7000\n",
    "LLM_args = {\"do_sample\":False, \"do_sample\":0, \n",
    "            \"entity_type_extraction_length\":100,\n",
    "            \"pad_token_id\":LLM.tokenizer.eos_token_id,\n",
    "            \"entity_extraction_length\":2000,\n",
    "            \"entity_condition_length\":10,\n",
    "            \"update_entity_description\":500,\n",
    "            \"subgraph_matching_length\": 10,\n",
    "            \"community_sum_length\":1000,\n",
    "            \"max_new_tokens\":600,\n",
    "            \"temperature\":0, \"filling_max_neighbor\":5}\n",
    "\n",
    "llm_config = {\n",
    "    \"engine\": LLM,\n",
    "    \"settings\": LLM_args,\n",
    "    \"chunk_size\": 2000,\n",
    "    \"overlap_size\":400 # overlap tokens between chunks\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b475df61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-23 19:42:22,627 [INFO] Checkpoint found at checkpoint\\meta_info.json, loading existing meta information.\n",
      "2024-12-23 19:42:22,767 [INFO] Successfully loaded meta information and graphs from checkpoint.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_info successfully loaded from checkpoint\\meta_info.json\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT_PATH = Path(\"checkpoint/meta_info.json\")\n",
    "EXTRACTION_PROMPT_PATH = Path('entity_extraction_prompt.txt')\n",
    "TYPE_EXTRACTION_PROMPT_PATH = Path('entity_type_extraction_prompt.txt')\n",
    "folder_path = 'input'\n",
    "meta_info, org_graph, updated_graph = load_indexing(folder_path, (LLM, LLM_args), CHECKPOINT_PATH=CHECKPOINT_PATH)\n",
    "llm_config['entity_type'] = meta_info['entity_type']\n",
    "# Improvement by merging discreate node\n",
    "concat_info = merge_discrete_nodes_into_major_graph(updated_graph, (LLM, LLM_args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86daf131",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# full-fill the missing entry/relationship based on neighbor nodes\n",
    "# Node name is not accurate need re-fine\n",
    "new_graph = fill_missing_graph(concat_info['concat_graph'], llm_config)\n",
    "concat_info['concat_graph'] = new_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224b1d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain node dataframe\n",
    "nodes_df, edges_df = obtain_node_edge_df(concat_info['concat_graph'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bce1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "community_meta_info = load_community(\n",
    "    concat_info = concat_info,\n",
    "    nodes_df = nodes_df,\n",
    "    edges_df = edges_df, \n",
    "    llm_config = llm_config,\n",
    "    seg_args = seg_args,\n",
    "    length_limit = length_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e765af22",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_nodes_df, community_df = process_communities(nodes_df, community_meta_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013f54f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_nodes_df.to_csv('checkpoint/nodes_info.csv', index=False)\n",
    "community_df.to_csv('checkpoint/community_info.csv', index=False)\n",
    "edges_df.to_csv('checkpoint/edge_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ea83b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CRA_LLM",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
