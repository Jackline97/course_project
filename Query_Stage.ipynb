{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49fb6ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda\\envs\\CRA_LLM\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import re\n",
    "import faiss\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import logging\n",
    "import numpy as np\n",
    "import secrets\n",
    "from typing import List, Tuple, Dict, Union, Optional, Any\n",
    "from transformers import pipeline, Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aab750f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_faiss_index_and_metadata(\n",
    "    save_folder: str = \"checkpoint\",\n",
    "    target_type: str = \"node\",\n",
    "    metadata_output_path: str = \"metadata.pkl\"\n",
    ") -> (faiss.Index, list):\n",
    "    embedding_output_path = f\"faiss_{target_type}_index.index\"\n",
    "    # Load FAISS index\n",
    "    index_path = os.path.join(save_folder, embedding_output_path)\n",
    "    if not os.path.exists(index_path):\n",
    "        raise FileNotFoundError(f\"FAISS index file not found at '{index_path}'.\")\n",
    "    print(f\"Loading FAISS index from '{index_path}'...\")\n",
    "    index = faiss.read_index(index_path)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92697159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_answer(LLM, input_info, message_mode=False, do_sample=True, temperature = 0.5, max_new_tokens=20, pad_token_id=None):\n",
    "    if not pad_token_id:\n",
    "        pad_token_id = LLM.tokenizer.eos_token_id\n",
    "        \n",
    "    if message_mode:\n",
    "        messages = input_info\n",
    "    else:\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": input_info},\n",
    "        ]\n",
    "    outputs = LLM(\n",
    "        messages,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample = do_sample,\n",
    "        temperature = temperature,\n",
    "        pad_token_id=pad_token_id\n",
    "    )\n",
    "    res = outputs[0][\"generated_text\"][-1]\n",
    "    torch.cuda.empty_cache()\n",
    "    return res['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63c33214",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_llm_model(\n",
    "    model_name: str = \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    task: str = \"text-generation\",\n",
    "    torch_dtype: torch.dtype = torch.bfloat16,\n",
    "    device_map: str = \"auto\"\n",
    ") -> Pipeline:\n",
    "    \"\"\"\n",
    "    Load a Language Model (LLM) using HuggingFace's pipeline.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The identifier of the model to load.\n",
    "        task (str): The task for the pipeline (e.g., \"text-generation\").\n",
    "        torch_dtype (torch.dtype): The data type for the model's tensors.\n",
    "        device_map (str): Specifies how to allocate model layers on devices.\n",
    "\n",
    "    Returns:\n",
    "        Pipeline: An instance of HuggingFace's Pipeline configured for the specified task.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Loading LLM model '{model_name}' for task '{task}' with dtype '{torch_dtype}' and device_map '{device_map}'.\")\n",
    "        llm = pipeline(\n",
    "            task=task,\n",
    "            model=model_name,\n",
    "            torch_dtype=torch_dtype,\n",
    "            device_map=device_map\n",
    "        )\n",
    "        logger.info(\"LLM model loaded successfully.\")\n",
    "        return llm\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading LLM model '{model_name}': {e}\")\n",
    "        raise\n",
    "\n",
    "def load_embedding_model(\n",
    "    model_name: str = \"stella_en_400M_v5\",\n",
    "    trust_remote_code: bool = True,\n",
    "    device: Optional[str] = None\n",
    ") -> SentenceTransformer:\n",
    "    \"\"\"\n",
    "    Load a SentenceTransformer model.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The identifier of the SentenceTransformer model to load.\n",
    "        trust_remote_code (bool): Whether to trust remote code from the model repository.\n",
    "        device (Optional[str]): The device to load the model on ('cuda', 'cpu', etc.). If None, auto-detects.\n",
    "\n",
    "    Returns:\n",
    "        SentenceTransformer: An instance of SentenceTransformer loaded with the specified model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if device is None:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            logger.info(f\"CUDA availability detected. Using device '{device}' for embedding model.\")\n",
    "        else:\n",
    "            logger.info(f\"Using specified device '{device}' for embedding model.\")\n",
    "\n",
    "        logger.info(f\"Loading embedding model '{model_name}' with trust_remote_code={trust_remote_code}.\")\n",
    "        embedding_model = SentenceTransformer(model_name, trust_remote_code=trust_remote_code)\n",
    "        embedding_model = embedding_model.to(device)\n",
    "        logger.info(\"Embedding model loaded successfully.\")\n",
    "        return embedding_model\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading embedding model '{model_name}': {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cb0ddd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(\n",
    "    model: SentenceTransformer,\n",
    "    sentence: Union[str, List[str]],\n",
    "    device: str = 'cuda'  # Default device\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Encode a sentence or a list of sentences into embeddings using the provided model.\n",
    "\n",
    "    Args:\n",
    "        model (SentenceTransformer): The sentence transformer model to use for encoding.\n",
    "        sentence (Union[str, List[str]]): A single sentence or a list of sentences to encode.\n",
    "        device (str, optional): The device to perform encoding on ('cpu' or 'cuda'). Defaults to 'cuda'.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The resulting embeddings as a NumPy array of type float32.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the input sentence is neither a string nor a list of strings.\n",
    "        RuntimeError: If encoding fails due to model or device issues.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate input type\n",
    "        if isinstance(sentence, str):\n",
    "            sentences = [sentence]\n",
    "        elif isinstance(sentence, list) and all(isinstance(s, str) for s in sentence):\n",
    "            sentences = sentence\n",
    "        else:\n",
    "            raise ValueError(\"Input 'sentence' must be a string or a list of strings.\")\n",
    "        \n",
    "        logger.info(f\"Encoding {len(sentences)} sentence(s) on device '{device}'.\")\n",
    "        \n",
    "        # Perform encoding\n",
    "        embeddings = model.encode(\n",
    "            sentences,\n",
    "            device=device,\n",
    "            convert_to_numpy=True,\n",
    "            show_progress_bar=False  # Disable progress bar for cleaner logs\n",
    "        ).astype(np.float32)\n",
    "        torch.cuda.empty_cache()\n",
    "        # Ensure consistent shape for single and multiple sentences\n",
    "        if len(embeddings.shape) == 1:\n",
    "            embeddings = embeddings.reshape(1, -1)\n",
    "        logger.info(f\"Encoding successful. Shape of embeddings: {embeddings.shape}.\")\n",
    "        return embeddings\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to encode sentence(s): {e}\")\n",
    "        raise RuntimeError(f\"Encoding failed: {e}\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c5ded24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens(text: str, tokenizer) -> int:\n",
    "    \"\"\"\n",
    "    Return the number of tokens in a string using the specified tokenizer.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input string.\n",
    "        tokenizer: Tokenizer to use for encoding the text.\n",
    "\n",
    "    Returns:\n",
    "        int: Number of tokens in the string.\n",
    "    \"\"\"\n",
    "    return len(tokenizer.encode(text, add_special_tokens=False))\n",
    "\n",
    "def expand_query(query: str, reports: List[str], llm_config: Dict) -> Tuple[str, Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Expand the query using a random community report template.\n",
    "\n",
    "    Args:\n",
    "        query (str): The original search query.\n",
    "        reports (List[str]): List of report templates to base the expansion on.\n",
    "        llm_config (Dict): Configuration for the LLM, including engine and settings.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, Dict[str, int]]: Expanded query text and the token usage details.\n",
    "    \"\"\"\n",
    "    # Validate inputs\n",
    "    if not reports:\n",
    "        log.warning(\"Reports list is empty. Returning the original query.\")\n",
    "        return query, {\"llm_calls\": 0, \"prompt_tokens\": 0, \"output_tokens\": 0}\n",
    "\n",
    "    if \"engine\" not in llm_config or \"settings\" not in llm_config:\n",
    "        log.error(\"Invalid LLM configuration. Missing 'engine' or 'settings'.\")\n",
    "        return query, {\"llm_calls\": 0, \"prompt_tokens\": 0, \"output_tokens\": 0}\n",
    "\n",
    "    # Extract LLM engine and settings\n",
    "    LLM_engine = llm_config[\"engine\"]\n",
    "    LLM_settings = llm_config[\"settings\"]\n",
    "\n",
    "    # Select a random template\n",
    "    template = secrets.choice(reports)\n",
    "\n",
    "    # Construct the prompt\n",
    "    prompt = (\n",
    "        f\"Create a hypothetical answer to the following query: {query}\\n\\n\"\n",
    "        f\"Format it to follow the structure of the template below:\\n\\n\"\n",
    "        f\"{template}\\n\"\n",
    "        \"Ensure that the hypothetical answer does not reference new named entities \"\n",
    "        \"that are not present in the original query.\"\n",
    "    )\n",
    "\n",
    "    # Generate the text using the LLM\n",
    "    try:\n",
    "        text = agent_answer(\n",
    "            LLM_engine,\n",
    "            prompt,\n",
    "            do_sample=LLM_settings[\"do_sample\"],\n",
    "            max_new_tokens=LLM_settings[\"max_new_tokens\"],\n",
    "            temperature=LLM_settings[\"temperature\"]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        log.error(\"LLM generation failed: %s\", str(e))\n",
    "        return query, {\"llm_calls\": 0, \"prompt_tokens\": 0, \"output_tokens\": 0}\n",
    "\n",
    "    # Token usage tracking\n",
    "    prompt_tokens = num_tokens(prompt, LLM_engine.tokenizer)\n",
    "    output_tokens = num_tokens(text, LLM_engine.tokenizer)\n",
    "    token_ct = {\n",
    "        \"llm_calls\": 1,\n",
    "        \"prompt_tokens\": prompt_tokens,\n",
    "        \"output_tokens\": output_tokens,\n",
    "    }\n",
    "\n",
    "    # Handle empty LLM response\n",
    "    if not text.strip():\n",
    "        log.warning(\"Failed to generate expansion for query: %s\", query)\n",
    "        return query, token_ct\n",
    "\n",
    "    return text, token_ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4b91843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def faiss_index_to_numpy(index):\n",
    "    \"\"\"\n",
    "    Converts a Faiss IndexFlatL2 object to a NumPy array.\n",
    "\n",
    "    Parameters:\n",
    "    - index (faiss.IndexFlatL2): The Faiss index to convert.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: A 2D NumPy array where each row is a vector from the index.\n",
    "    \"\"\"\n",
    "    # Ensure the index is an instance of IndexFlatL2\n",
    "    if not isinstance(index, faiss.IndexFlatL2):\n",
    "        raise TypeError(\"The provided index is not an instance of faiss.IndexFlatL2\")\n",
    "\n",
    "    # Get the dimensionality and total number of vectors\n",
    "    d = index.d\n",
    "    ntotal = index.ntotal\n",
    "\n",
    "    # Pre-allocate a NumPy array to hold all vectors\n",
    "    vectors = np.zeros((ntotal, d), dtype='float32')\n",
    "\n",
    "    # Reconstruct each vector and store it in the NumPy array\n",
    "    for i in range(ntotal):\n",
    "        index.reconstruct(i, vectors[i])\n",
    "\n",
    "    return vectors\n",
    "def build_faiss_index(embeddings: np.ndarray, index_type: str = \"IndexFlatIP\") -> faiss.Index:\n",
    "    \"\"\"\n",
    "    Build a FAISS index from document embeddings.\n",
    "\n",
    "    Args:\n",
    "        embeddings (np.ndarray): 2D array of normalized document embeddings.\n",
    "        index_type (str, optional): Type of FAISS index. Defaults to \"IndexFlatIP\".\n",
    "\n",
    "    Returns:\n",
    "        faiss.Index: FAISS index.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dimension = embeddings.shape[1]\n",
    "        if index_type == \"IndexFlatIP\":\n",
    "            index = faiss.IndexFlatIP(dimension)\n",
    "            logger.info(\"Using FAISS IndexFlatIP for inner product similarity.\")\n",
    "        elif index_type == \"IndexFlatL2\":\n",
    "            index = faiss.IndexFlatL2(dimension)\n",
    "            logger.info(\"Using FAISS IndexFlatL2 for L2 distance similarity.\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported index type: {index_type}\")\n",
    "\n",
    "        index.add(embeddings)\n",
    "        logger.info(f\"FAISS index built with {index.ntotal} vectors.\")\n",
    "        return index\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error building FAISS index: {e}\")\n",
    "        raise\n",
    "\n",
    "def normalize_embeddings(embeddings: np.ndarray) -> np.ndarray:\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    # Avoid division by zero\n",
    "    norms[norms == 0] = 1\n",
    "    normalized_embeddings = embeddings / norms\n",
    "    logger.info(\"Normalized embeddings to unit vectors.\")\n",
    "    return normalized_embeddings\n",
    "\n",
    "def compute_cosine_similarity(\n",
    "    query_embedding: np.ndarray,\n",
    "    index: faiss.Index,\n",
    "    top_k: int = 5\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    try:\n",
    "        # Normalize the query embedding\n",
    "        query_norm = np.linalg.norm(query_embedding)\n",
    "        if query_norm == 0:\n",
    "            logger.warning(\"Query embedding has zero norm. Returning empty results.\")\n",
    "            return np.array([]), np.array([])\n",
    "\n",
    "        normalized_query = query_embedding / query_norm\n",
    "        normalized_query = normalized_query.astype(np.float32)\n",
    "        distances, indices = index.search(normalized_query, top_k)\n",
    "        logger.info(f\"Retrieved top-{top_k} similar documents.\")\n",
    "        return distances, indices\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error computing cosine similarity: {e}\")\n",
    "        raise\n",
    "\n",
    "def build_context(query_embedding, embed_data, target_df, idx_col = 'community_key', top_k = 5):\n",
    "    document_embeddings = faiss_index_to_numpy(embed_data)\n",
    "    normalized_document_embeddings = normalize_embeddings(document_embeddings)\n",
    "    faiss_index = build_faiss_index(normalized_document_embeddings, index_type=\"IndexFlatIP\")\n",
    "    distances, indices = compute_cosine_similarity(query_embedding, faiss_index, top_k=top_k)\n",
    "    if indices.size == 0:\n",
    "        logger.warning(\"No similar documents found.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Validate indices\n",
    "    max_index = len(target_df) - 1\n",
    "    if np.any(indices > max_index) or np.any(indices < 0):\n",
    "        raise ValueError(\"Some indices are out of bounds of the community dataframe.\")\n",
    "\n",
    "    selected_nodes = target_df[target_df[idx_col].isin(indices[0])].reset_index(drop=True)\n",
    "    return selected_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcd2b2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt import drift_search_system_prompt\n",
    "\n",
    "import re\n",
    "import json\n",
    "\n",
    "def extract_json(text):\n",
    "    \"\"\"\n",
    "    Extracts the JSON content from a markdown code block.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text containing the JSON code block.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted JSON string.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If no JSON code block is found.\n",
    "    \"\"\"\n",
    "    pattern = r\"```json\\s*(\\{.*?\\})\\s*```\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        raise ValueError(\"JSON code block not found.\")\n",
    "\n",
    "def clean_json(json_str):\n",
    "    \"\"\"\n",
    "    Cleans the extracted JSON string to make it valid for parsing.\n",
    "    Specifically handles multi-line strings with triple quotes.\n",
    "\n",
    "    Args:\n",
    "        json_str (str): The raw JSON string extracted from the code block.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned JSON string.\n",
    "    \"\"\"\n",
    "    # Replace triple quotes with escaped double quotes\n",
    "    json_str = json_str.replace('\"\"\"', '\"')\n",
    "\n",
    "    # Use regex to find the value of \"intermediate_answer\" and escape necessary characters\n",
    "    def replace_multiline(match):\n",
    "        content = match.group(1)\n",
    "        # Escape backslashes and double quotes\n",
    "        content = content.replace('\\\\', '\\\\\\\\').replace('\"', '\\\\\"')\n",
    "        # Replace newlines with escaped newline characters\n",
    "        content = content.replace('\\n', '\\\\n')\n",
    "        return f'\"intermediate_answer\": \"{content}\"'\n",
    "\n",
    "    json_str = re.sub(\n",
    "        r'\"intermediate_answer\":\\s*\"([^\"]*)\"', \n",
    "        replace_multiline, \n",
    "        json_str, \n",
    "        flags=re.DOTALL\n",
    "    )\n",
    "    return json_str\n",
    "\n",
    "def parse_json(cleaned_json_str):\n",
    "    return json.loads(cleaned_json_str)\n",
    "\n",
    "def obtain_task_json(text):\n",
    "    raw_json = extract_json(text)\n",
    "    cleaned_json = clean_json(raw_json)\n",
    "    data = parse_json(cleaned_json)\n",
    "    return data\n",
    "\n",
    "def decompose_query(\n",
    "        query: str, reports: pd.DataFrame, llm_config: dict\n",
    "    ) -> tuple[dict, dict[str, int]]:\n",
    "        \"\"\"\n",
    "        Decompose the query into subqueries based on the fetched global structures.\n",
    "\n",
    "        Args:\n",
    "            query (str): The original search query.\n",
    "            reports (pd.DataFrame): DataFrame containing community reports.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple[dict, int, int]: Parsed response and the number of prompt and output tokens used.\n",
    "        \"\"\"\n",
    "        LLM_engine = llm_config[\"engine\"]\n",
    "        LLM_settings = llm_config[\"settings\"]\n",
    "        community_reports = \"\\n\\n\".join(reports[\"content\"].tolist())\n",
    "        prompt = drift_search_system_prompt.DRIFT_PRIMER_PROMPT.format(\n",
    "            query=query, community_reports=community_reports\n",
    "        )\n",
    "        text = agent_answer(\n",
    "            LLM_engine,\n",
    "            prompt,\n",
    "            do_sample=LLM_settings[\"do_sample\"],\n",
    "            max_new_tokens=LLM_settings[\"max_new_tokens\"],\n",
    "            temperature=LLM_settings[\"temperature\"]\n",
    "        )\n",
    "        token_ct = {\n",
    "            \"llm_calls\": 1,\n",
    "            \"prompt_tokens\": num_tokens(prompt, LLM_engine.tokenizer),\n",
    "            \"output_tokens\": num_tokens(text, LLM_engine.tokenizer),\n",
    "        }\n",
    "        return text, token_ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "491a6481",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_df = pd.read_csv('checkpoint/nodes_info.csv')\n",
    "edge_df = pd.read_csv('checkpoint/edge_info.csv')\n",
    "community_df = pd.read_csv('checkpoint/community_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee3c57c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_df['content'] = [i.replace(\"Here is a comprehensive summary of the data:\\n\\n\", \"\") for i in node_df['content'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e29ef0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FAISS index from 'checkpoint\\faiss_node_index.index'...\n",
      "Loading FAISS index from 'checkpoint\\faiss_edge_index.index'...\n",
      "Loading FAISS index from 'checkpoint\\faiss_graph_index.index'...\n"
     ]
    }
   ],
   "source": [
    "node_description_embed = load_faiss_index_and_metadata(target_type='node')\n",
    "edge_description_embed = load_faiss_index_and_metadata(target_type='edge')\n",
    "community_description_embed = load_faiss_index_and_metadata(target_type='graph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e32a800",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-22 22:33:31,611 [INFO] __main__: Loading LLM model 'meta-llama/Meta-Llama-3-8B-Instruct' for task 'text-generation' with dtype 'torch.bfloat16' and device_map 'auto'.\n",
      "2024-12-22 22:33:31,902 [INFO] accelerate.utils.modeling: We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 4/4 [00:06<00:00,  1.55s/it]\n",
      "2024-12-22 22:33:38,463 [INFO] __main__: LLM model loaded successfully.\n",
      "2024-12-22 22:33:38,464 [INFO] __main__: CUDA availability detected. Using device 'cuda' for embedding model.\n",
      "2024-12-22 22:33:38,464 [INFO] __main__: Loading embedding model 'stella_en_400M_v5' with trust_remote_code=True.\n",
      "2024-12-22 22:33:38,466 [INFO] sentence_transformers.SentenceTransformer: Use pytorch device_name: cuda\n",
      "2024-12-22 22:33:38,467 [INFO] sentence_transformers.SentenceTransformer: Load pretrained SentenceTransformer: stella_en_400M_v5\n",
      "2024-12-22 22:33:38,632 [WARNING] xformers: A matching Triton is not available, some optimizations will not be enabled\n",
      "Traceback (most recent call last):\n",
      "  File \"F:\\anaconda\\envs\\CRA_LLM\\lib\\site-packages\\xformers\\__init__.py\", line 57, in _is_triton_available\n",
      "    import triton  # noqa\n",
      "ModuleNotFoundError: No module named 'triton'\n",
      "Some weights of the model checkpoint at stella_en_400M_v5 were not used when initializing NewModel: ['new.pooler.dense.bias', 'new.pooler.dense.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2024-12-22 22:33:39,628 [INFO] sentence_transformers.SentenceTransformer: 2 prompts are loaded, with the keys: ['s2p_query', 's2s_query']\n",
      "2024-12-22 22:33:39,630 [INFO] __main__: Embedding model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "LLM = load_llm_model()\n",
    "embedding_model = load_embedding_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42f01a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_community_json(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts JSON content from a given text string and validates required fields.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text containing JSON.\n",
    "\n",
    "    Returns:\n",
    "        dict: The extracted JSON as a dictionary if valid, else empty dict.\n",
    "    \"\"\"\n",
    "    # Regex to find JSON within code blocks\n",
    "    code_block_pattern = r\"```json\\s*(\\{.*?\\})\\s*```\"\n",
    "    match = re.search(code_block_pattern, text, re.DOTALL)\n",
    "\n",
    "    if match:\n",
    "        json_str = match.group(1)\n",
    "    else:\n",
    "        # If no code block, attempt to find JSON directly\n",
    "        json_start = text.find('{')\n",
    "        json_end = text.rfind('}')\n",
    "        if json_start == -1 or json_end == -1:\n",
    "            raise ValueError(\"No JSON object found in the response.\")\n",
    "        json_str = text[json_start:json_end+1]\n",
    "\n",
    "    # Parse the JSON string\n",
    "    data = json.loads(json_str)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d78bbd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "class DriftAction:\n",
    "    \"\"\"\n",
    "    Represent an action containing a query, answer, score, and follow-up actions.\n",
    "\n",
    "    This class encapsulates action strings produced by the LLM in a structured way.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        query: str,\n",
    "        answer: str | None = None,\n",
    "        follow_ups: list[\"DriftAction\"] | None = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the DriftAction with a query, optional answer, and follow-up actions.\n",
    "\n",
    "        Args:\n",
    "            query (str): The query for the action.\n",
    "            answer (Optional[str]): The answer to the query, if available.\n",
    "            follow_ups (Optional[list[DriftAction]]): A list of follow-up actions.\n",
    "        \"\"\"\n",
    "        self.query = query\n",
    "        self.answer: str | None = answer  # Corresponds to an 'intermediate_answer'\n",
    "        self.score: float | None = None\n",
    "        self.follow_ups: list[DriftAction] = (\n",
    "            follow_ups if follow_ups is not None else []\n",
    "        )\n",
    "        \n",
    "    @property\n",
    "    def is_complete(self) -> bool:\n",
    "        \"\"\"Check if the action is complete (i.e., an answer is available).\"\"\"\n",
    "        return self.answer is not None\n",
    "\n",
    "    def get_sorted_frequency_dict(self, numbers):\n",
    "        frequency = Counter(numbers)\n",
    "        sorted_items = sorted(frequency.items(), key=lambda item: (-item[1], item[0]))\n",
    "        sorted_frequency_dict = dict(sorted_items)\n",
    "        return sorted_frequency_dict\n",
    "\n",
    "    def locate_specific_row(self, edge_df, source_node, target_node, use_ids=False):\n",
    "        if use_ids:\n",
    "            mask = (edge_df['source_node_id'] == source_node) & (edge_df['target_node_id'] == target_node)\n",
    "        else:\n",
    "            mask = (edge_df['source_node'] == source_node) & (edge_df['target_node'] == target_node)\n",
    "\n",
    "        matched_rows = edge_df[mask]\n",
    "        return matched_rows\n",
    "    \n",
    "    def get_neighbors(self, edge_df, node_name):\n",
    "        neighbors_as_source = edge_df[edge_df['source_node'] == node_name]['target_node']\n",
    "        neighbors_as_target = edge_df[edge_df['target_node'] == node_name]['source_node']\n",
    "        neighbors = pd.concat([neighbors_as_source, neighbors_as_target]).unique()\n",
    "        return neighbors.tolist()\n",
    "    \n",
    "    \n",
    "    def _get_header(self, attributes: list[str]) -> list[str]:\n",
    "        header = [\"id\", \"title\"]\n",
    "        attributes = [col for col in attributes if col not in header]\n",
    "        attributes = [col for col in attributes if col != 'rating']\n",
    "        header.extend(attributes)\n",
    "        return header \n",
    "        \n",
    "    def serialize(self, include_follow_ups: bool = True) -> dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Serialize the action to a dictionary.\n",
    "\n",
    "        Args:\n",
    "            include_follow_ups (bool): Whether to include follow-up actions in the serialization.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict[str, Any]\n",
    "            Serialized action as a dictionary.\n",
    "        \"\"\"\n",
    "        data = {\n",
    "            \"query\": self.query,\n",
    "            \"answer\": self.answer,\n",
    "            \"score\": self.score,\n",
    "        }\n",
    "        if include_follow_ups:\n",
    "            data[\"follow_ups\"] = [action.serialize() for action in self.follow_ups]\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def asearch(self,\n",
    "                global_query: str,\n",
    "                llm_config:dict,\n",
    "                node_embed: Any,\n",
    "                node_df: pd.DataFrame,\n",
    "                edge_df: pd.DataFrame,\n",
    "                community_df: pd.DataFrame,\n",
    "                top_k: int = 10,\n",
    "                neighbor_top_k: int = 5,\n",
    "                scorer: Any = None):\n",
    "        \n",
    "        LLM_engine = llm_config[\"engine\"]\n",
    "        LLM_settings = llm_config[\"settings\"]\n",
    "        max_context_size = llm_config[\"max_context_size\"]\n",
    "        final_content = []\n",
    "        cur_length = 0\n",
    "        query_embed = encode_sentence(model=llm_config['embedding'], sentence=self.query)\n",
    "        # obtain relevant entities\n",
    "        selected_entities = build_context(query_embedding = query_embed, \n",
    "                                          embed_data = node_embed,target_df = node_df, \n",
    "                                          idx_col = 'node_id', top_k = top_k)\n",
    "        \n",
    "        # check these entity corresponding community and sort by number of appearance\n",
    "        # DFS community selection\n",
    "        community_rank = {}\n",
    "        for level in range(1, 4):\n",
    "            cur_level = selected_entities[f\"Community_Level_{level}\"]\n",
    "            for idx, com_key in enumerate(cur_level):\n",
    "                if not math.isnan(com_key):\n",
    "                    if idx not in community_rank:\n",
    "                        community_rank[idx] = [com_key]\n",
    "                    else:\n",
    "                        community_rank[idx].append(com_key)\n",
    "\n",
    "        DFS_community_choice = [int(community_rank[key][-1]) for key in community_rank if community_rank[key]]\n",
    "        sorted_community = self.get_sorted_frequency_dict(DFS_community_choice)\n",
    "        \n",
    "        # obtain releveant communities report to the entities and maintain within max_length\n",
    "        community_reports_str = [\n",
    "            content\n",
    "            for key in sorted_community.keys()\n",
    "            for content in community_df.loc[community_df['community_key'] == key, 'content'].astype(str).tolist()]\n",
    "        \n",
    "        community_reports = [extract_community_json(i) for i in community_reports_str]\n",
    "\n",
    "        sorted_keys = list(sorted_community.keys())\n",
    "        community_reports = [\n",
    "            {**item, \"id\": sorted_keys[idx]}\n",
    "            for idx, item in enumerate(community_reports)\n",
    "        ]\n",
    "\n",
    "        attributes = (\n",
    "            list(community_reports[0].keys())\n",
    "            if community_reports[0].keys()\n",
    "            else []\n",
    "        )\n",
    "\n",
    "        header = self._get_header(attributes)\n",
    "        concatenated_community_report = \"\\n\".join(\n",
    "            \".\".join(str(report[attr]) for attr in header)\n",
    "            for report in community_reports\n",
    "        )\n",
    "        head_line = \",\".join(header)\n",
    "        concatenated_community_report = \"-----Report-----\\n\" + f\"{head_line}\\n\" + concatenated_community_report\n",
    "        report_len = num_tokens(concatenated_community_report, LLM_engine.tokenizer)\n",
    "        if cur_length + report_len < max_context_size:\n",
    "            final_content.append(concatenated_community_report)        \n",
    "        \n",
    "        \n",
    "        # return the selected entity information\n",
    "        concatenated_entity_report = '\\n'.join(\n",
    "            f\"{row.node_id},{row.node_name},{row.content}\"\n",
    "            for row in selected_entities.itertuples(index=False)\n",
    "        )\n",
    "        \n",
    "        node_len = num_tokens(concatenated_entity_report, LLM_engine.tokenizer)\n",
    "        if cur_length + node_len < max_context_size:\n",
    "            final_content.append(concatenated_entity_report)        \n",
    "        \n",
    "        concatenated_entity_report = \"-----Entities-----\\n\" + \"id,entity,description\\n\" +concatenated_entity_report\n",
    "        entity_list = selected_entities[\"node_name\"].tolist()\n",
    "        neighbor_set = []\n",
    "        neighbor_info = {}\n",
    "        for entity_name in entity_list:\n",
    "            cur_neighbor = self.get_neighbors(edge_df, entity_name)[:neighbor_top_k]\n",
    "            for nei in cur_neighbor:\n",
    "                if (entity_name, nei) not in neighbor_set and (nei, entity_name) not in neighbor_set:\n",
    "                    neighbor_set.append((entity_name, nei))\n",
    "                    target_info = self.locate_specific_row(edge_df, entity_name, nei)\n",
    "                    if len(target_info) == 0:\n",
    "                        target_info = self.locate_specific_row(edge_df, nei, entity_name)\n",
    "                    neighbor_info[(entity_name, nei)] = target_info['content'].iloc[0]\n",
    "\n",
    "        entity_len = num_tokens(concatenated_entity_report, LLM_engine.tokenizer)\n",
    "        # return the selected entity relationship information\n",
    "        concatenated_edge_report = '\\n'.join(\n",
    "            f\"{edge[0]},{edge[1]},{neighbor_info[edge]}\"\n",
    "            for edge in neighbor_info)\n",
    "        \n",
    "        concatenated_edge_report = \"-----Relationships-----\\n\" + \"id,source,target,description\\n\" +concatenated_edge_report\n",
    "        \n",
    "        edge_len = num_tokens(concatenated_edge_report, LLM_engine.tokenizer)\n",
    "        if cur_length + edge_len < max_context_size:\n",
    "            final_content.append(concatenated_edge_report)\n",
    "        \n",
    "        context_result = \"\\n\\n\".join(final_content)\n",
    "        # TODO, add text unit info\n",
    "        \n",
    "        search_prompt = drift_search_system_prompt.DRIFT_LOCAL_SYSTEM_PROMPT.format(\n",
    "            context_data=context_result,\n",
    "            response_type=\"multiple paragraphs\",\n",
    "            global_query=global_query,\n",
    "        )\n",
    "        search_messages = [\n",
    "            {\"role\": \"system\", \"content\": search_prompt},\n",
    "            {\"role\": \"user\", \"content\": self.query},\n",
    "        ]        \n",
    "\n",
    "        llm_response = agent_answer(\n",
    "            LLM_engine,\n",
    "            input_info=search_messages,\n",
    "            message_mode=True,\n",
    "            do_sample=LLM_settings[\"do_sample\"],\n",
    "            max_new_tokens=LLM_settings[\"max_new_tokens\"],\n",
    "            temperature=LLM_settings[\"temperature\"]\n",
    "        )\n",
    "\n",
    "        parsed_output = parse_llm_output(llm_response)\n",
    "        \n",
    "        self.answer = parsed_output['sections']\n",
    "        self.follow_ups = parsed_output['follow_up_questions']\n",
    "        self.score = parsed_output['score']\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c47eb597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "import random\n",
    "class QueryState:\n",
    "    \"\"\"Manage the state of the query, including a graph of actions.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.graph = nx.MultiDiGraph()\n",
    "\n",
    "    def add_action(self, action: DriftAction, metadata: dict[str, Any] | None = None):\n",
    "        \"\"\"Add an action to the graph with optional metadata.\"\"\"\n",
    "        self.graph.add_node(action, **(metadata or {}))\n",
    "        \n",
    "    def relate_actions(\n",
    "        self, parent: DriftAction, child: DriftAction, weight: float = 1.0\n",
    "    ):\n",
    "        \"\"\"Relate two actions in the graph.\"\"\"\n",
    "        self.graph.add_edge(parent, child, weight=weight)\n",
    "        \n",
    "    def add_all_follow_ups(\n",
    "        self,\n",
    "        action: DriftAction,\n",
    "        follow_ups: list[DriftAction] | list[str],\n",
    "        weight: float = 1.0,\n",
    "    ):\n",
    "        \"\"\"Add all follow-up actions and links them to the given action.\"\"\"\n",
    "        if len(follow_ups) == 0:\n",
    "            logger.warning(\"No follow-up actions for action: %s\", action.query)\n",
    "\n",
    "        for follow_up in follow_ups:\n",
    "            if isinstance(follow_up, str):\n",
    "                follow_up = DriftAction(query=follow_up)\n",
    "            elif not isinstance(follow_up, DriftAction):\n",
    "                logger.warning(\n",
    "                    \"Follow-up action is not a string, found type: %s\", type(follow_up)\n",
    "                )\n",
    "\n",
    "            self.add_action(follow_up)\n",
    "            self.relate_actions(action, follow_up, weight)\n",
    "            \n",
    "    def find_incomplete_actions(self) -> list[DriftAction]:\n",
    "        \"\"\"Find all unanswered actions in the graph.\"\"\"\n",
    "        return [node for node in self.graph.nodes if not node.is_complete]\n",
    "    \n",
    "    def rank_incomplete_actions(\n",
    "        self, scorer: Callable[[DriftAction], float] | None = None\n",
    "    ) -> list[DriftAction]:\n",
    "        \"\"\"Rank all unanswered actions in the graph if scorer available.\"\"\"\n",
    "        unanswered = self.find_incomplete_actions()\n",
    "        if scorer:\n",
    "            for node in unanswered:\n",
    "                node.compute_score(scorer)\n",
    "            return sorted(\n",
    "                unanswered,\n",
    "                key=lambda node: (\n",
    "                    node.score if node.score is not None else float(\"-inf\")\n",
    "                ),\n",
    "                reverse=True,\n",
    "            )\n",
    "\n",
    "        # shuffle the list if no scorer\n",
    "        random.shuffle(unanswered)\n",
    "        return list(unanswered)\n",
    "    \n",
    "    def serialize(\n",
    "        self, include_context: bool = False\n",
    "    ) -> dict[str, Any] | tuple[dict[str, Any], dict[str, Any], str]:\n",
    "        \"\"\"Serialize the graph to a dictionary, including nodes and edges.\"\"\"\n",
    "        # Create a mapping from nodes to unique IDs\n",
    "        node_to_id = {node: idx for idx, node in enumerate(self.graph.nodes())}\n",
    "\n",
    "        # Serialize nodes\n",
    "        nodes: list[dict[str, Any]] = [\n",
    "            {\n",
    "                **node.serialize(include_follow_ups=False),\n",
    "                \"id\": node_to_id[node],\n",
    "                **self.graph.nodes[node],\n",
    "            }\n",
    "            for node in self.graph.nodes()\n",
    "        ]\n",
    "\n",
    "        # Serialize edges\n",
    "        edges: list[dict[str, Any]] = [\n",
    "            {\n",
    "                \"source\": node_to_id[u],\n",
    "                \"target\": node_to_id[v],\n",
    "                \"weight\": edge_data.get(\"weight\", 1.0),\n",
    "            }\n",
    "            for u, v, edge_data in self.graph.edges(data=True)\n",
    "        ]\n",
    "        return {\"nodes\": nodes, \"edges\": edges}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba6c7b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_llm_output(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Parse the LLM output into a structured dictionary.\n",
    "    \"\"\"\n",
    "    # This dictionary will hold all parsed content\n",
    "    parsed_output = {\n",
    "        \"title\": None,\n",
    "        \"sections\": {},        # key: section heading, value: content\n",
    "        \"references\": None,\n",
    "        \"score\": None,\n",
    "        \"follow_up_questions\": []\n",
    "    }\n",
    "\n",
    "    # Regex patterns\n",
    "    # 1) Headings: lines that start and end with double asterisks (**Heading**)\n",
    "    heading_pattern = re.compile(r\"\\*\\*(.+?)\\*\\*\")\n",
    "    # 2) Score pattern: \"**Score:** some_number\"\n",
    "    score_pattern = re.compile(r\"\\*\\*Score:\\*\\*\\s*(\\d+)\")\n",
    "    # 3) Follow-up questions heading\n",
    "    followup_pattern = re.compile(r\"\\*\\*Follow-up Questions:\\*\\*\")\n",
    "\n",
    "    # Split the text into lines\n",
    "    lines = text.splitlines()\n",
    "\n",
    "    # A helper to track which heading we are currently in\n",
    "    current_heading = None\n",
    "    # Buffer to hold text lines for the current heading\n",
    "    section_lines = []\n",
    "\n",
    "    # We’ll manually detect references, score, and follow-up questions as we go\n",
    "    in_followup_section = False\n",
    "\n",
    "    for line in lines:\n",
    "        line_stripped = line.strip()\n",
    "\n",
    "        # Check if we've matched a heading\n",
    "        heading_match = heading_pattern.match(line_stripped)\n",
    "        # Check for follow-up questions heading\n",
    "        followup_match = followup_pattern.match(line_stripped)\n",
    "        # Check for score\n",
    "        score_match = score_pattern.match(line_stripped)\n",
    "\n",
    "        # If line is empty, just skip it\n",
    "        if not line_stripped:\n",
    "            continue\n",
    "\n",
    "        # If it's the \"Score:\" line\n",
    "        if score_match:\n",
    "            parsed_output[\"score\"] = int(score_match.group(1))\n",
    "            continue\n",
    "\n",
    "        # If it's the \"Follow-up Questions:\" heading\n",
    "        if followup_match:\n",
    "            # Save the current heading's content (if any) before clearing\n",
    "            if current_heading and section_lines:\n",
    "                parsed_output[\"sections\"][current_heading] = \"\\n\".join(section_lines).strip()\n",
    "                section_lines = []\n",
    "            current_heading = \"Follow-up Questions\"\n",
    "            in_followup_section = True\n",
    "            continue\n",
    "\n",
    "        # If it’s any other heading\n",
    "        if heading_match and not followup_match:\n",
    "            # Save the previous heading’s content in the dictionary\n",
    "            if current_heading and section_lines:\n",
    "                parsed_output[\"sections\"][current_heading] = \"\\n\".join(section_lines).strip()\n",
    "                section_lines = []\n",
    "\n",
    "            # The new heading\n",
    "            current_heading = heading_match.group(1).strip()\n",
    "            # Check if it's 'References'\n",
    "            if current_heading.lower() == \"references\":\n",
    "                in_followup_section = False  # references is separate\n",
    "            else:\n",
    "                in_followup_section = False\n",
    "            continue\n",
    "\n",
    "        # If we're in the follow-up questions section, each line is a potential question\n",
    "        if in_followup_section:\n",
    "            # Typically, follow-up questions are numbered lines. Let's grab them.\n",
    "            # You can refine this logic if your format changes.\n",
    "            parsed_output[\"follow_up_questions\"].append(line_stripped)\n",
    "        else:\n",
    "            # Otherwise, this line belongs to the current heading's content\n",
    "            section_lines.append(line_stripped)\n",
    "\n",
    "    # After the loop, we still might have leftover content for the last heading\n",
    "    if current_heading and section_lines and current_heading != \"Follow-up Questions\":\n",
    "        content = \"\\n\".join(section_lines).strip()\n",
    "        # If the heading is 'References', store them separately\n",
    "        if current_heading.lower() == \"references\":\n",
    "            parsed_output[\"references\"] = content\n",
    "        else:\n",
    "            parsed_output[\"sections\"][current_heading] = content\n",
    "\n",
    "    # The first heading might logically be the \"title\" if the structure calls for that.\n",
    "    # If you consider the very first heading in the text to be the title, you can do:\n",
    "    all_headings = list(parsed_output[\"sections\"].keys())\n",
    "    if all_headings:\n",
    "        parsed_output[\"title\"] = all_headings[0]\n",
    "        first_heading_content = parsed_output[\"sections\"].pop(all_headings[0])\n",
    "        parsed_output[\"sections\"][\"intro\"] = first_heading_content\n",
    "\n",
    "    return parsed_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4d64054",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def run_drift_loop(llm_config, query_state, query, node_description_embed, node_df, edge_df, community_df):\n",
    "    \"\"\"\n",
    "    Executes the DRIFT loop based on the provided configuration and state.\n",
    "\n",
    "    Args:\n",
    "        llm_config (dict): Configuration dictionary containing 'query_epoch' and 'drift_k_followups'.\n",
    "        query_state: An object managing the state of queries and actions.\n",
    "        query (str): The global query string.\n",
    "        node_description_embed: Embedding for node descriptions.\n",
    "        node_df (DataFrame): DataFrame containing node information.\n",
    "        edge_df (DataFrame): DataFrame containing edge information.\n",
    "        community_df (DataFrame): DataFrame containing community information.\n",
    "    \"\"\"\n",
    "    # Configure the logger\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    if not logger.handlers:\n",
    "        # Prevent adding multiple handlers in interactive environments\n",
    "        handler = logging.StreamHandler()\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        logger.addHandler(handler)\n",
    "\n",
    "    max_epochs = llm_config.get('query_epoch', 0)\n",
    "    drift_k_followups = llm_config.get('drift_k_followups', 0)\n",
    "\n",
    "    if max_epochs <= 0:\n",
    "        logger.warning(\"No epochs to run. Exiting DRIFT loop.\")\n",
    "        return\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        actions = query_state.rank_incomplete_actions()\n",
    "        if not actions:\n",
    "            logger.info(\"No more actions to take. Exiting DRIFT loop.\")\n",
    "            break\n",
    "\n",
    "        # Select top k follow-up actions\n",
    "        selected_actions = actions[:drift_k_followups]\n",
    "        if not selected_actions:\n",
    "            logger.info(\"No actions selected after applying drift_k_followups. Exiting DRIFT loop.\")\n",
    "            break\n",
    "\n",
    "        logger.debug(f\"Epoch {epoch}: Processing {len(selected_actions)} actions.\")\n",
    "\n",
    "        # Function to process a single action with error handling\n",
    "        def process_action(action):\n",
    "            try:\n",
    "                return action.asearch(\n",
    "                    global_query=query,\n",
    "                    llm_config=llm_config,\n",
    "                    node_embed=node_description_embed,\n",
    "                    node_df=node_df,\n",
    "                    edge_df=edge_df,\n",
    "                    community_df=community_df\n",
    "                )\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing action {action}: {e}\")\n",
    "                return None\n",
    "\n",
    "        results = []\n",
    "        # Use ThreadPoolExecutor to process actions in parallel if asearch is I/O bound\n",
    "        with ThreadPoolExecutor(max_workers=drift_k_followups) as executor:\n",
    "            future_to_action = {executor.submit(process_action, action): action for action in selected_actions}\n",
    "            for future in as_completed(future_to_action):\n",
    "                action_result = future.result()\n",
    "                if action_result:\n",
    "                    results.append(action_result)\n",
    "\n",
    "        if not results:\n",
    "            logger.warning(f\"Epoch {epoch}: No successful action results. Continuing to next epoch.\")\n",
    "            continue\n",
    "\n",
    "        # Update the query state with new actions and their follow-ups\n",
    "        for action in results:\n",
    "            query_state.add_action(action)\n",
    "            follow_ups = getattr(action, 'follow_ups', None)\n",
    "            if follow_ups:\n",
    "                query_state.add_all_follow_ups(action, follow_ups)\n",
    "            else:\n",
    "                logger.warning(f\"Action {action} has no 'follow_ups' attribute.\")\n",
    "\n",
    "        logger.info(f\"Finished Epoch {epoch}/{max_epochs} Stage\")\n",
    "\n",
    "    logger.info(\"DRIFT loop completed.\")\n",
    "    return query_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfe7499d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_args = {\"do_sample\":False, \"do_sample\":0, \n",
    "            \"entity_type_extraction_length\":100,\n",
    "            \"pad_token_id\":LLM.tokenizer.eos_token_id,\n",
    "            \"entity_extraction_length\":2000,\n",
    "            \"entity_condition_length\":10,\n",
    "            \"update_entity_description\":500,\n",
    "            \"subgraph_matching_length\": 10,\n",
    "            \"community_sum_length\":1000,\n",
    "            \"max_new_tokens\":2000,\n",
    "            \"temperature\":0, \"filling_max_neighbor\":8}\n",
    "\n",
    "llm_config = {\n",
    "    \"engine\": LLM,\n",
    "    \"settings\": LLM_args,\n",
    "    \"embedding\": embedding_model,\n",
    "    \"query_epoch\":2,\n",
    "    \"drift_k_followups\":3,\n",
    "    \"max_context_size\":6000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a28dc4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import math\n",
    "\n",
    "def build_graph(response_state):\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    for node_info in response_state.get('nodes', []):\n",
    "        node_id = node_info.get('id')\n",
    "        if node_id is None:\n",
    "            continue\n",
    "\n",
    "        answer_value = node_info.get('answer', '')\n",
    "        if answer_value:\n",
    "            # Use `or 0.0` to replace None with 0.0, then convert to float\n",
    "            score_value = node_info.get('score') or 0.0\n",
    "            score_value = float(score_value)\n",
    "\n",
    "            G.add_node(\n",
    "                node_id,\n",
    "                query=node_info.get('query'),\n",
    "                answer=answer_value,\n",
    "                score=score_value\n",
    "            )\n",
    "\n",
    "    for edge_info in response_state.get('edges', []):\n",
    "        src = edge_info.get('source')\n",
    "        tgt = edge_info.get('target')\n",
    "        if not src or not tgt:\n",
    "            continue\n",
    "        if src in G and tgt in G:\n",
    "            weight = edge_info.get('weight', 0.0)\n",
    "            G.add_edge(src, tgt, weight=weight)\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "def find_roots_and_leaves(G):\n",
    "    roots = [n for n, deg in G.in_degree() if deg == 0]\n",
    "    leaves = [n for n, deg in G.out_degree() if deg == 0]\n",
    "    return roots, leaves\n",
    "\n",
    "\n",
    "def best_path_by_node_score(G):\n",
    "    roots, leaves = find_roots_and_leaves(G)\n",
    "    best_path = None\n",
    "    best_score = -math.inf\n",
    "\n",
    "    for root in roots:\n",
    "        for leaf in leaves:\n",
    "            for path in nx.all_simple_paths(G, root, leaf):\n",
    "                # Convert each node’s 'score' to float in case of any leftover weirdness\n",
    "                path_score = sum(float(G.nodes[n].get('score', 0.0)) for n in path)\n",
    "                if path_score > best_score:\n",
    "                    best_score = path_score\n",
    "                    best_path = path\n",
    "\n",
    "    return best_path, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e6bcdc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query, graph, llm_config, use_all=True):\n",
    "    \"\"\"\n",
    "    Generates an answer for a given user query based on relevant information from a graph.\n",
    "\n",
    "    :param query:        The user query (string).\n",
    "    :param graph:        A graph structure containing relevant information in its node attributes.\n",
    "    :param llm_config:   A dictionary containing LLM engine and settings.\n",
    "    :param use_all:      Boolean flag indicating whether to use all nodes or a best path subset.\n",
    "    :return:             A string containing the final answer from the LLM.\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt template\n",
    "    prompt_template = (\n",
    "        \"You are provided with the following user query:\\n\"\n",
    "        \"\\\"{query}\\\"\\n\\n\"\n",
    "        \"Along with relevant information:\\n\"\n",
    "        \"{content}\\n\\n\"\n",
    "        \"Please analyze the query and the provided information to generate a clear, concise, \"\n",
    "        \"and accurate final answer. Please only generate the answer!\"\n",
    "    )\n",
    "\n",
    "    # Extract engine and settings from configuration\n",
    "    LLM_engine = llm_config[\"engine\"]\n",
    "    LLM_settings = llm_config[\"settings\"]\n",
    "\n",
    "    # Determine which nodes to use\n",
    "    if use_all:\n",
    "        nodes_to_use = list(graph.nodes())\n",
    "    else:\n",
    "        best_path, best_score = best_path_by_node_score(graph)\n",
    "        nodes_to_use = list(best_path)\n",
    "\n",
    "    # Aggregate relevant info from each node\n",
    "    content_snippets = []\n",
    "    for node in nodes_to_use:\n",
    "        cur_info = graph.nodes[node]  # Retrieve node attributes\n",
    "        content_snippets.append(f\"Query: {cur_info['query']}\\nAnswer: {cur_info['answer']}\\n\")\n",
    "\n",
    "    # Construct final content for the LLM prompt\n",
    "    final_output = \"\".join(content_snippets)\n",
    "\n",
    "    # Format the full prompt\n",
    "    cur_prompt = prompt_template.format(query=query, content=final_output)\n",
    "\n",
    "    # Call your LLM agent with the constructed prompt\n",
    "    llm_response = agent_answer(\n",
    "        LLM_engine,\n",
    "        input_info=cur_prompt,  # Pass the prompt here\n",
    "        message_mode=False,\n",
    "        do_sample=LLM_settings[\"do_sample\"],\n",
    "        max_new_tokens=LLM_settings[\"max_new_tokens\"],\n",
    "        temperature=LLM_settings[\"temperature\"]\n",
    "    )\n",
    "    return llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3094f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-22 22:33:54,669 [INFO] __main__: Encoding 1 sentence(s) on device 'cuda'.\n",
      "2024-12-22 22:33:54,730 [INFO] __main__: Encoding successful. Shape of embeddings: (1, 8192).\n",
      "2024-12-22 22:33:54,733 [INFO] __main__: Normalized embeddings to unit vectors.\n",
      "2024-12-22 22:33:54,733 [INFO] __main__: Using FAISS IndexFlatIP for inner product similarity.\n",
      "2024-12-22 22:33:54,734 [INFO] __main__: FAISS index built with 56 vectors.\n",
      "2024-12-22 22:33:54,739 [INFO] __main__: Retrieved top-5 similar documents.\n"
     ]
    }
   ],
   "source": [
    "query = \"How to apply transformers for medical data classficaiton tasks?\"\n",
    "reports = list(community_df['content'])\n",
    "augmented_query, token_ct = expand_query(query, reports=reports, llm_config=llm_config)\n",
    "query_embed = encode_sentence(model=llm_config['embedding'], sentence=augmented_query)\n",
    "selected_context = build_context(query_embedding = query_embed, \n",
    "                                 embed_data = community_description_embed, \n",
    "                                 target_df = community_df, idx_col = 'community_key', top_k = 5)\n",
    "# add self-correction is tasks is not in json format\n",
    "tasks, token_ct = decompose_query(query, reports=selected_context, llm_config=llm_config)\n",
    "parsed_output = obtain_task_json(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2d18947",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_action = DriftAction(query=query, \n",
    "                     follow_ups=parsed_output.get(\"follow_up_queries\", []),\n",
    "                     answer=parsed_output.get(\"intermediate_answer\"))\n",
    "init_action.score = parsed_output.get(\"score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c88f1216",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_state = QueryState()\n",
    "query_state.add_action(init_action)\n",
    "query_state.add_all_follow_ups(init_action, init_action.follow_ups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38e7c816",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-22 22:34:10,952 - INFO - Encoding 1 sentence(s) on device 'cuda'.\n",
      "2024-12-22 22:34:10,952 [INFO] __main__: Encoding 1 sentence(s) on device 'cuda'.\n",
      "2024-12-22 22:34:10,952 - INFO - Encoding 1 sentence(s) on device 'cuda'.\n",
      "2024-12-22 22:34:10,954 - INFO - Encoding 1 sentence(s) on device 'cuda'.\n",
      "2024-12-22 22:34:10,952 [INFO] __main__: Encoding 1 sentence(s) on device 'cuda'.\n",
      "2024-12-22 22:34:10,954 [INFO] __main__: Encoding 1 sentence(s) on device 'cuda'.\n",
      "2024-12-22 22:34:11,009 - INFO - Encoding successful. Shape of embeddings: (1, 8192).\n",
      "2024-12-22 22:34:11,009 [INFO] __main__: Encoding successful. Shape of embeddings: (1, 8192).\n",
      "2024-12-22 22:34:11,010 - INFO - Encoding successful. Shape of embeddings: (1, 8192).\n",
      "2024-12-22 22:34:11,010 [INFO] __main__: Encoding successful. Shape of embeddings: (1, 8192).\n",
      "2024-12-22 22:34:11,018 - INFO - Encoding successful. Shape of embeddings: (1, 8192).\n",
      "2024-12-22 22:34:11,018 [INFO] __main__: Encoding successful. Shape of embeddings: (1, 8192).\n",
      "2024-12-22 22:34:11,023 - INFO - Normalized embeddings to unit vectors.\n",
      "2024-12-22 22:34:11,023 [INFO] __main__: Normalized embeddings to unit vectors.\n",
      "2024-12-22 22:34:11,024 - INFO - Using FAISS IndexFlatIP for inner product similarity.\n",
      "2024-12-22 22:34:11,024 [INFO] __main__: Using FAISS IndexFlatIP for inner product similarity.\n",
      "2024-12-22 22:34:11,024 - INFO - Normalized embeddings to unit vectors.\n",
      "2024-12-22 22:34:11,024 [INFO] __main__: Normalized embeddings to unit vectors.\n",
      "2024-12-22 22:34:11,025 - INFO - Using FAISS IndexFlatIP for inner product similarity.\n",
      "2024-12-22 22:34:11,025 [INFO] __main__: Using FAISS IndexFlatIP for inner product similarity.\n",
      "2024-12-22 22:34:11,028 - INFO - FAISS index built with 324 vectors.\n",
      "2024-12-22 22:34:11,028 [INFO] __main__: FAISS index built with 324 vectors.\n",
      "2024-12-22 22:34:11,029 - INFO - Normalized embeddings to unit vectors.\n",
      "2024-12-22 22:34:11,029 [INFO] __main__: Normalized embeddings to unit vectors.\n",
      "2024-12-22 22:34:11,029 - INFO - Retrieved top-10 similar documents.\n",
      "2024-12-22 22:34:11,029 [INFO] __main__: Retrieved top-10 similar documents.\n",
      "2024-12-22 22:34:11,029 - INFO - Using FAISS IndexFlatIP for inner product similarity.\n",
      "2024-12-22 22:34:11,029 - INFO - FAISS index built with 324 vectors.\n",
      "2024-12-22 22:34:11,029 [INFO] __main__: Using FAISS IndexFlatIP for inner product similarity.\n",
      "2024-12-22 22:34:11,029 [INFO] __main__: FAISS index built with 324 vectors.\n",
      "2024-12-22 22:34:11,035 - INFO - Retrieved top-10 similar documents.\n",
      "2024-12-22 22:34:11,035 [INFO] __main__: Retrieved top-10 similar documents.\n",
      "2024-12-22 22:34:11,040 - INFO - FAISS index built with 324 vectors.\n",
      "2024-12-22 22:34:11,040 [INFO] __main__: FAISS index built with 324 vectors.\n",
      "2024-12-22 22:34:11,045 - INFO - Retrieved top-10 similar documents.\n",
      "2024-12-22 22:34:11,045 [INFO] __main__: Retrieved top-10 similar documents.\n",
      "2024-12-22 22:35:50,049 - INFO - Finished Epoch 1/2 Stage\n",
      "2024-12-22 22:35:50,049 [INFO] __main__: Finished Epoch 1/2 Stage\n",
      "2024-12-22 22:35:50,053 - INFO - Encoding 1 sentence(s) on device 'cuda'.\n",
      "2024-12-22 22:35:50,053 [INFO] __main__: Encoding 1 sentence(s) on device 'cuda'.\n",
      "2024-12-22 22:35:50,054 - INFO - Encoding 1 sentence(s) on device 'cuda'.\n",
      "2024-12-22 22:35:50,054 [INFO] __main__: Encoding 1 sentence(s) on device 'cuda'.\n",
      "2024-12-22 22:35:50,056 - INFO - Encoding 1 sentence(s) on device 'cuda'.\n",
      "2024-12-22 22:35:50,056 [INFO] __main__: Encoding 1 sentence(s) on device 'cuda'.\n",
      "2024-12-22 22:35:50,111 - INFO - Encoding successful. Shape of embeddings: (1, 8192).\n",
      "2024-12-22 22:35:50,111 [INFO] __main__: Encoding successful. Shape of embeddings: (1, 8192).\n",
      "2024-12-22 22:35:50,111 - INFO - Encoding successful. Shape of embeddings: (1, 8192).\n",
      "2024-12-22 22:35:50,111 [INFO] __main__: Encoding successful. Shape of embeddings: (1, 8192).\n",
      "2024-12-22 22:35:50,118 - INFO - Encoding successful. Shape of embeddings: (1, 8192).\n",
      "2024-12-22 22:35:50,118 [INFO] __main__: Encoding successful. Shape of embeddings: (1, 8192).\n",
      "2024-12-22 22:35:50,127 - INFO - Normalized embeddings to unit vectors.\n",
      "2024-12-22 22:35:50,127 [INFO] __main__: Normalized embeddings to unit vectors.\n",
      "2024-12-22 22:35:50,127 - INFO - Normalized embeddings to unit vectors.\n",
      "2024-12-22 22:35:50,127 [INFO] __main__: Normalized embeddings to unit vectors.\n",
      "2024-12-22 22:35:50,128 - INFO - Using FAISS IndexFlatIP for inner product similarity.\n",
      "2024-12-22 22:35:50,128 [INFO] __main__: Using FAISS IndexFlatIP for inner product similarity.\n",
      "2024-12-22 22:35:50,128 - INFO - Using FAISS IndexFlatIP for inner product similarity.\n",
      "2024-12-22 22:35:50,129 - INFO - Normalized embeddings to unit vectors.\n",
      "2024-12-22 22:35:50,128 [INFO] __main__: Using FAISS IndexFlatIP for inner product similarity.\n",
      "2024-12-22 22:35:50,129 [INFO] __main__: Normalized embeddings to unit vectors.\n",
      "2024-12-22 22:35:50,131 - INFO - Using FAISS IndexFlatIP for inner product similarity.\n",
      "2024-12-22 22:35:50,131 [INFO] __main__: Using FAISS IndexFlatIP for inner product similarity.\n",
      "2024-12-22 22:35:50,132 - INFO - FAISS index built with 324 vectors.\n",
      "2024-12-22 22:35:50,132 [INFO] __main__: FAISS index built with 324 vectors.\n",
      "2024-12-22 22:35:50,133 - INFO - FAISS index built with 324 vectors.\n",
      "2024-12-22 22:35:50,133 [INFO] __main__: FAISS index built with 324 vectors.\n",
      "2024-12-22 22:35:50,134 - INFO - Retrieved top-10 similar documents.\n",
      "2024-12-22 22:35:50,134 [INFO] __main__: Retrieved top-10 similar documents.\n",
      "2024-12-22 22:35:50,134 - INFO - Retrieved top-10 similar documents.\n",
      "2024-12-22 22:35:50,134 [INFO] __main__: Retrieved top-10 similar documents.\n",
      "2024-12-22 22:35:50,134 - INFO - FAISS index built with 324 vectors.\n",
      "2024-12-22 22:35:50,134 [INFO] __main__: FAISS index built with 324 vectors.\n",
      "2024-12-22 22:35:50,143 - INFO - Retrieved top-10 similar documents.\n",
      "2024-12-22 22:35:50,143 [INFO] __main__: Retrieved top-10 similar documents.\n",
      "2024-12-22 22:36:56,247 - WARNING - Action <__main__.DriftAction object at 0x00000253CECEEDD0> has no 'follow_ups' attribute.\n",
      "2024-12-22 22:36:56,247 [WARNING] __main__: Action <__main__.DriftAction object at 0x00000253CECEEDD0> has no 'follow_ups' attribute.\n",
      "2024-12-22 22:36:56,248 - INFO - Finished Epoch 2/2 Stage\n",
      "2024-12-22 22:36:56,248 [INFO] __main__: Finished Epoch 2/2 Stage\n",
      "2024-12-22 22:36:56,249 - INFO - DRIFT loop completed.\n",
      "2024-12-22 22:36:56,249 [INFO] __main__: DRIFT loop completed.\n"
     ]
    }
   ],
   "source": [
    "updated_query_state = run_drift_loop(llm_config, query_state, query, node_description_embed, node_df, edge_df, community_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4bb5d941",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_state = query_state.serialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2ffe92ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_graph = build_graph(response_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2a6b4ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    }
   ],
   "source": [
    "final_answer1 = generate_answer(query, answer_graph, llm_config, use_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1ac4e01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_answer2 = generate_answer(query, answer_graph, llm_config, use_all=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7e9375df",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Applying Transformers for Medical Data Classification Tasks**\n",
      "\n",
      "Transformers have shown great promise in various machine learning applications, including natural language processing, computer vision, and bioinformatics. In the context of medical data classification tasks, transformers can be used to develop accurate and efficient models for disease diagnosis, treatment planning, and patient outcome prediction.\n",
      "\n",
      "Transformers can be applied to medical data classification tasks in several ways, including text classification, image classification, and time series analysis. They offer several advantages, such as improved accuracy, efficient processing, and flexibility. However, there are still several challenges and future directions to explore, including data quality, domain adaptation, and explainability.\n",
      "\n",
      "Transformers can be used to improve the interpretability of medical data classification models by analyzing attention mechanisms and using explainable AI (XAI) techniques. They can also be used to improve the robustness of medical data classification models to noisy or missing data by developing new transformer architectures or combining them with other machine learning techniques.\n",
      "\n",
      "In addition, transformers can be used to classify medical data that involves both sequential and graph-structured data by using a hybrid approach that combines sequential and graph transformers. However, there are still several challenges to be addressed, including developing more efficient and scalable transformer architectures and incorporating domain-specific knowledge into the models.\n",
      "\n",
      "Overall, transformers have the potential to revolutionize medical data classification tasks, enabling more accurate diagnoses, personalized treatment plans, and improved patient outcomes.\n"
     ]
    }
   ],
   "source": [
    "print(final_answer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "53dad542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers can be applied to medical data classification tasks, particularly those involving sequential data such as medical text, time-series data, or genomic sequences. They can also be used to classify graph-structured data, such as medical images or protein-protein interaction networks. For medical data that involves both sequential and graph-structured data, a hybrid approach can be taken, combining sequential and graph transformers to leverage the strengths of both architectures. However, transformers may not be well-suited for large-scale medical datasets or complex relationships between variables, and future directions include developing more efficient and scalable architectures and incorporating domain-specific knowledge.\n"
     ]
    }
   ],
   "source": [
    "print(final_answer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de373dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CRA_LLM",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
